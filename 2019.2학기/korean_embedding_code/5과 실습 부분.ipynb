{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec, ldamulticore\n",
    "from gensim import corpora\n",
    "from konlpy.tag import Okt, Komoran, Hannanum, Kkma, Mecab\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "#mecab = Mecab(dicpath=\"C:/mecab/mecab-ko-dic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fasttext 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LG\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\LG\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Creating the model\n",
    "ko_model = KeyedVectors.load_word2vec_format('wiki.ko.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the tokens \n",
    "words = []\n",
    "for word in ko_model.vocab:\n",
    "    words.append(word)\n",
    "\n",
    "# Printing out number of tokens available\n",
    "print(\"Number of Tokens: {}\".format(len(words)))\n",
    "\n",
    "# Printing out the dimension of a word vector \n",
    "print(\"Dimension of a word vector: {}\".format(\n",
    "    len(ko_model[words[0]])\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the vector of a word \n",
    "print(\"Vector components of a word: {}\".format(\n",
    "    ko_model[words[0]]\n",
    "))\n",
    "\n",
    "print(words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-trained 된 fasttext 모델로 단어간 유사도 검색을 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a word \n",
    "find_similar_to = '사랑'\n",
    "\n",
    "# Finding out similar words [default= top 10]\n",
    "for similar_word in ko_model.similar_by_word(find_similar_to):\n",
    "    print(\"Word: {0}, Similarity: {1:.2f}\".format(\n",
    "        similar_word[0], similar_word[1]\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test words \n",
    "word_add = ['동물', '파충류']\n",
    "word_sub = ['뱀']\n",
    "\n",
    "# Word vector addition and subtraction \n",
    "for resultant_word in ko_model.most_similar(\n",
    "    positive=word_add, negative=word_sub\n",
    "):\n",
    "    print(\"Word : {0} , Similarity: {1:.2f}\".format(\n",
    "        resultant_word[0], resultant_word[1]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wi.ko.vec에 저장된 88만여개 단어 중 선두 300개 단어를 2차원 상에 맵핑하면 다음과 같은 그림이 보여짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc, rcParams\n",
    "font_name = font_manager.FontProperties(fname=\"MALGUN.TTF\").get_name()\n",
    "rc('font', family=font_name)\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "%matplotlib inline\n",
    "\n",
    "# Limit number of tokens to be visualized\n",
    "limit = 300\n",
    "vector_dim = 300\n",
    "\n",
    "# Getting tokens and vectors\n",
    "words = []\n",
    "embedding = np.array([])\n",
    "i = 0\n",
    "for word in ko_model.vocab:\n",
    "    # Break the loop if limit exceeds \n",
    "    if i == limit: break\n",
    "\n",
    "    # Getting token \n",
    "    words.append(word)\n",
    "\n",
    "    # Appending the vectors \n",
    "    embedding = np.append(embedding, ko_model[word])\n",
    "\n",
    "    i += 1\n",
    "\n",
    "# Reshaping the embedding vector \n",
    "embedding = embedding.reshape(limit, vector_dim)\n",
    "\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(10, 4),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "\n",
    "# Creating the tsne plot [Warning: will take time]\n",
    "tsne = TSNE(perplexity=30.0, n_components=2, init='pca', n_iter=5000)\n",
    "\n",
    "low_dim_embedding = tsne.fit_transform(embedding)\n",
    "\n",
    "# Finally plotting and saving the fig \n",
    "plot_with_labels(low_dim_embedding, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = ko_model.wv.most_similar(positive=['동물', '파충류'], negative=['뱀'])\n",
    "print(similarities)\n",
    "\n",
    "not_matching = ko_model.wv.doesnt_match(\"아침 점심 저녁 된장국\".split())\n",
    "print(not_matching)\n",
    "\n",
    "sim_score = ko_model.wv.similarity('컴퓨터', '인간')\n",
    "print(sim_score)\n",
    "\n",
    "sim_score = ko_model.wv.similarity('로봇', '인간')\n",
    "print(sim_score)\n",
    "\n",
    "sim_score = ko_model.wv.similarity('사랑해', '사랑의')\n",
    "print(sim_score)\n",
    "\n",
    "print(ko_model.wv.most_similar('전자'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m = Mecab.Tagger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(tokenizer_name):\n",
    "    if tokenizer_name == \"komoran\":\n",
    "        tokenizer = Komoran()\n",
    "    elif tokenizer_name == \"okt\":\n",
    "        tokenizer = Okt()\n",
    "    elif tokenizer_name == \"mecab\":\n",
    "        tokenizer = Mecab()\n",
    "    elif tokenizer_name == \"hannanum\":\n",
    "        tokenizer = Hannanum()\n",
    "    elif tokenizer_name == \"kkma\":\n",
    "        tokenizer = Kkma()\n",
    "    elif tokenizer_name == \"khaiii\":\n",
    "        tokenizer = KhaiiiApi()\n",
    "    else:\n",
    "        tokenizer = Mecab()\n",
    "    return tokenizer\n",
    "\n",
    "class Doc2VecInput:\n",
    "\n",
    "    def __init__(self, fname, tokenizer_name='hannanum'):\n",
    "        self.fname = fname\n",
    "        self.tokenizer = get_tokenizer(tokenizer_name)\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.fname, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    sentence, movie_id = line.strip().split(\"\\u241E\")\n",
    "                    tokens = self.tokenizer.morphs(sentence)\n",
    "                    tagged_doc = TaggedDocument(words=tokens, tags=['movie_%s' % movie_id])\n",
    "                    yield tagged_doc\n",
    "                except:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_fname = \"C:/work/BOAZ embeding/test.txt\"\n",
    "output_fname = \"C:/work/BOAZ embeding/doc2vec.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Doc2VecInput(corpus_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "종합 평점은 4점 드립니다.␞92575\n",
      "\n",
      "--------------------------------------------------------------------------------------------\n",
      "원작이 칭송받는 이유는 웹툰 계 자체의 질적 저하가 심각하기 때문.  원작이나 영화나 별로인건 마찬가지.␞92575\n",
      "\n",
      "--------------------------------------------------------------------------------------------\n",
      "나름의  감동도 있고 안타까운 마음에 가슴도 먹먹  배우들의 연기가 good 김수현 최고~␞92575\n",
      "\n",
      "--------------------------------------------------------------------------------------------\n",
      "이런걸 돈주고 본 내자신이 후회스럽다 최악의 쓰레기 영화 김수현 밖에없는 저질 삼류영화␞92575\n",
      "\n",
      "--------------------------------------------------------------------------------------------\n",
      "총 sentence의 개수 : 5145개\n"
     ]
    }
   ],
   "source": [
    "with open(\"C:/work/BOAZ embeding/test.txt\", 'rt', encoding='UTF8') as f:\n",
    "    print(f.readline())\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(f.readline())\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(f.readline())\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(f.readline())\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    count = 4\n",
    "    for sentence in f:\n",
    "        count+=1\n",
    "    print(\"총 sentence의 개수 : {}개\".format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델에 사용된 감상평은 총 5145개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(corpus, dm = 1, vector_size = 100)\n",
    "model.save(output_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dm이 1이면 PV-DM, 0이면 PV-DBOW  \n",
    "vector_size 는 임베딩 벡터의 크기. 벡터 사이즈가 클수록 생성된 모델의 성능이 정교해지나 훈련 시간 및 메모리의 크기가 사이즈가 커진다는 단점이 있다. Glove 논문에 따르면 vector size가 커질수록 모델 성능이 좋아지는 것을 확인할 수 있습니다. vector 사이즈가 100까지 증가할 경우 가장 극적인 성능 향상을 보이고 그 이상의 사이즈일 경우 정확도 그래프가 완만하게 증가하는 것을 확인 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "import random\n",
    "\n",
    "class Doc2VecEvaluator:\n",
    "\n",
    "    def __init__(self, model_fname=\"data/doc2vec.vecs\", use_notebook=False):\n",
    "        self.model = Doc2Vec.load(model_fname)\n",
    "        self.doc2idx = {el:idx for idx, el in enumerate(self.model.docvecs.doctags.keys())}\n",
    "        self.use_notebook = use_notebook\n",
    "\n",
    "    def most_similar(self, movie_id, topn=10):\n",
    "        similar_movies = self.model.docvecs.most_similar('movie_' + str(movie_id), topn=topn)\n",
    "        for movie_id, score in similar_movies:\n",
    "            print(self.get_movie_title(movie_id), score)\n",
    "\n",
    "    def get_titles_in_corpus(self, n_sample=5):\n",
    "        movie_ids = random.sample(self.model.docvecs.doctags.keys(), n_sample)\n",
    "        return {movie_id: self.get_movie_title(movie_id) for movie_id in movie_ids}\n",
    "\n",
    "    def get_movie_title(self, movie_id):\n",
    "        url = 'http://movie.naver.com/movie/point/af/list.nhn?st=mcode&target=after&sword=%s' % movie_id.split(\"_\")[1]\n",
    "        resp = requests.get(url)\n",
    "        root = html.fromstring(resp.text)\n",
    "        try:\n",
    "            title = root.xpath('//div[@class=\"choice_movie_info\"]//h5//a/text()')[0]\n",
    "        except:\n",
    "            title = \"\"\n",
    "        return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fname=\"C:/work/BOAZ embeding/doc2vec.model\"\n",
    "model = Doc2VecEvaluator(model_fname) #모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_100677': '타임 투 러브', 'movie_120042': '야간비행', 'movie_74533': '119 구조대'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_titles_in_corpus(n_sample=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 리뷰에 맞는 영화 이름을 매칭 시킨 후(get_movie_title함수) 랜덤 영화 3개를 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "우리는 동물원을 샀다 0.9999254941940308\n",
      "뜨거운 녀석들 0.9999253153800964\n",
      "인디애니박스: 셀마의 단백질 커피 0.9999243021011353\n"
     ]
    }
   ],
   "source": [
    "model.most_similar(100677, topn = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100677번 영화(타임 투 러브)와 가장 유사한 영화 3개를 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 잠재 디리클레 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_fname = \"C:/work/BOAZ embeding/corrected_ratings_corpus.txt\"\n",
    "documents, tokenized_corpus = [], []\n",
    "tokenizer = get_tokenizer(\"hannanum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(corpus_fname, 'r', encoding='utf-8') as f:\n",
    "    for document in f:\n",
    "        tokens = list(set(tokenizer.morphs(document.strip())))\n",
    "        documents.append(document)\n",
    "        tokenized_corpus.append(tokens)\n",
    "\n",
    "dictionary = corpora.Dictionary(tokenized_corpus)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = ldamulticore.LdaMulticore(corpus, id2word = dictionary,\n",
    "                               num_topics = 30,\n",
    "                               workers = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_topics: 토픽수(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics = LDA.get_document_topics(corpus,\n",
    "                                    minimum_probability = 0.5,\n",
    "                                    per_word_topics = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.5미만의 토픽 분포는 무시한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [(27, 0.6311326)]\n",
      "1 [(7, 0.65424)]\n",
      "2 [(20, 0.7805107)]\n",
      "3 [(24, 0.87320346)]\n",
      "4 [(7, 0.93079644)]\n"
     ]
    }
   ],
   "source": [
    "for doc_idx, topic in enumerate(all_topics[:5]):\n",
    "    print(doc_idx, topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ex) 0번 문서는 30개의 토픽 중 27번째 토픽의 확률값이 가장 높다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fname = 'C:/work/BOAZ embeding/lda' #모델 저장\n",
    "with open(output_fname + \".results\", 'w', encoding = \"utf-8\") as f:\n",
    "    for doc_idx, topic in enumerate(all_topics):\n",
    "        if len(topic) == 1:\n",
    "            # tuple 형태로 되어있는 데이터로 가져와서 나눠줌\n",
    "            topic_id, prob = topic[0]\n",
    "            f.writelines(documents[doc_idx].strip() + \"\\u241E\" + ' '.join(tokenized_corpus[doc_idx]) + \"\\u241E\" + str(topic_id) + \"\\u241E\" + str(prob) + \"\\n\")\n",
    "LDA.save(output_fname + \".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "from collections import defaultdict\n",
    "\n",
    "class LDAEvaluator:\n",
    "\n",
    "    def __init__(self, model_path=\"./lda\", tokenizer_name=\"hannanum\"):\n",
    "        self.tokenizer = get_tokenizer(tokenizer_name)\n",
    "        self.all_topics = self.load_results(model_path + \".results\")\n",
    "        self.model = LdaModel.load(model_path + \".model\")\n",
    "\n",
    "    def load_results(self, results_fname):\n",
    "        topic_dict = defaultdict(list)\n",
    "        with open(results_fname, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                sentence, _, topic_id, prob = line.strip().split(\"\\u241E\")\n",
    "                topic_dict[int(topic_id)].append((sentence, float(prob)))\n",
    "        for key in topic_dict.keys():\n",
    "            topic_dict[key] = sorted(topic_dict[key], key=lambda x: x[1], reverse=True)\n",
    "        return topic_dict\n",
    "\n",
    "    def show_topic_docs(self, topic_id, topn=10):\n",
    "        return self.all_topics[topic_id][:topn]\n",
    "\n",
    "    def show_topic_words(self, topic_id, topn=10):\n",
    "        return self.model.show_topic(topic_id, topn=topn)\n",
    "\n",
    "    def show_new_document_topic(self, documents):\n",
    "        tokenized_documents = [self.tokenizer.morphs(document) for document in documents]\n",
    "        curr_corpus = [self.model.id2word.doc2bow(tokenized_document) for tokenized_document in tokenized_documents]\n",
    "        topics = self.model.get_document_topics(curr_corpus, minimum_probability=0.5, per_word_topics=False)\n",
    "        for doc_idx, topic in enumerate(topics):\n",
    "            if len(topic) == 1:\n",
    "                topic_id, prob = topic[0]\n",
    "                print(documents[doc_idx], \", topic id:\", str(topic_id), \", prob:\", str(prob))\n",
    "            else:\n",
    "                print(documents[doc_idx], \", there is no dominant topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LDAEvaluator('./lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('에릭은 당연히 나왔어야 했다. 완벽한 삶을 쫒는 이병헌, 허나 사람은 자기 인생이 가장 소중하고 제일 중요하다. 여기 나오는 모든 애들이 다 자기 잇속만 챙긴다. 에릭 또한, 자신의 복수를 위해서 이병헌이 뭔 짓을 하건 상관없이 죽인 것.이게 삶이다',\n",
       "  0.9817925),\n",
       " ('오히려 평점 조작같은데.. 감독이 충무로 미움 삿을수도 있다는 생각까지 든다...한국영화 별로 좋아하진 않지만.. 이영화가 이렇게까지 평점 낮을만큼 재미없진 않다. 이정도 평점의 다른 영화같지않은 영화들과 비교할때 이 평점은 이해가 되질 않는다.',\n",
       "  0.9816869),\n",
       " ('줄리아 로버츠의 웃음은 정말 보는 사람을 기쁘게 해준다. 웃음 하나로 기쁨과 슬픔을 표현할 수 있는 배우..전통을 깨고 여성의 새로운 삶을 살으라고 영화는 보여주는 데 영화에서는 전통을 완전히 벗어나야 한다는 것만을 강조하는 것 같아 아쉽다.',\n",
       "  0.98140126),\n",
       " ('고등학교때 보고 지금에서 생각난 영화 참~그땐 눈물 콧물 흘리면서 봤는데짐 보면 나올라나.. 하긴 세월이 많이 흘렀지 아마 짐 시대하곤 스토리자체가 뻔하니깐 하지만 지금도 잊혀지지 않는 장면은 주인공이 공부하는 장면과하늘도 갈라놓지 못한 그들의 사랑',\n",
       "  0.9808263),\n",
       " ('영화를 보는 사이사이에 아쉬움이 남기는 하지만、그나마 성은님께서 성은이 망극하게도 화끈하게 연기해 주어서 재미있게 잘 봤습니다。영화의 스토리로는 여타 영화와 다를바가 없지만 배슬기도 청순하게 연기 잘 했습니다。다음번에는 좀 더 용기내기를 바랍니다！',\n",
       "  0.97880334),\n",
       " ('급할수록 돌아가라는 말이 있다. 요즘같이 급하게 사는 세상속에.이런영화를 본다면,,인생은 급한게 아니라, 느린세상이 더 행복하다는걸 느끼게 해준다..',\n",
       "  0.9750976),\n",
       " ('스토리를 풀어나가기 힘든 주제임에도 침착하고 노골적으로 잘 표현한 영화라고 생각한다.. 실제로는 더 심각했다는 사실에 충격을 느낀다..',\n",
       "  0.9738683),\n",
       " ('지구는 인간들만이 지키는 것이 아니다라는 걸 가르쳐준 멋진 결말. 인간의 가치를 깨닫게 해주는 영화. 우리는 어째서 여기에 생존하고 있는가?',\n",
       "  0.97386354),\n",
       " ('BEST아무 것도 없을 땐 자신이 외로운 줄도 모른다사랑을 하면서 자신이 그동안 외로웠다는걸 깨닫는다그리고 사랑이 끝나면 외롭지 않을수도, 처음처럼 외로움을 모를수도 없게된다.',\n",
       "  0.9729122),\n",
       " ('4편까지 고수해 온 진행 방식과 스토리에서 탈피한 점은 좋지만, 나온 결과물은 재미도 감동도 별로 없고 캐릭터와 스토리는 난장판으로 진행되는 것 같다.',\n",
       "  0.97237635)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.show_topic_docs(topic_id = 0) #해당 토픽의 확률 값이 가장 높은 문서를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('이', 0.03160628),\n",
       " ('하', 0.02849265),\n",
       " ('.', 0.027684068),\n",
       " ('는', 0.025728267),\n",
       " ('ㄴ', 0.023062158),\n",
       " ('지', 0.020668844),\n",
       " ('을', 0.01825664),\n",
       " ('은', 0.017982338),\n",
       " ('ㄹ', 0.017367618),\n",
       " ('다', 0.017247446)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.show_topic_words(topic_id = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "너무 사랑스러운 영화 , topic id: 21 , prob: 0.8388832\n",
      "인생을 말하는 영화 , topic id: 13 , prob: 0.67638844\n"
     ]
    }
   ],
   "source": [
    "model.show_new_document_topic([\"너무 사랑스러운 영화\", \"인생을 말하는 영화\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "새로운 문서의 토픽을 확인했다. 문서를 형태소 분석한 뒤 이를 모델에 넣어 토픽을 추론하고, 가장 높은 확률값을 지니는 토픽 ID와 확률을 리턴."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
