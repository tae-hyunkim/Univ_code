{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 데이터 불러오기\n",
    "\n",
    "### 분석하고자 하는 데이터 로드하기 \n",
    "\n",
    "여기서는 네이버 영화 리뷰 데이터 활용하여 리뷰가 긍정인지 부정인지 판별하는 모델을 만들고자 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('https://github.com/e9t/nsmc/raw/master/ratings_train.txt')\n",
    "with open('nsmc_train.csv', 'wb') as f:\n",
    "    f.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsmc = pd.read_csv('nsmc_train.csv', sep='\\t')\n",
    "nsmc.dropna(inplace = True)\n",
    "nsmc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어 분석을 진행하기 위하여 형태소 단위의 토큰화 기법을 사용하여야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 형태소 분석기 만들기 - 지도학습\n",
    "\n",
    "Konlpy package 사이트 : \n",
    "https://konlpy-ko.readthedocs.io/ko/v0.4.3/ \n",
    "\n",
    "## Konlpy 패키지에서 사용할 수 있는 한국어 형태소 분석기는 다음과 같다.\n",
    "* komoran\n",
    "* kkma\n",
    "* Mecab\n",
    "* Twitter\n",
    "* Hannanum\n",
    "\n",
    "## 딥러닝 기반 형태소 분석기\n",
    "* stanfordnlp\n",
    "\n",
    "## 윈도우에서 사용 불가능한 모델\n",
    "* khaiii\n",
    "\n",
    "khaiii의 경우 사용하기 위해서 Colab을 활용하는 것이 좋다. 이때 사용하기 위한 코드는 khaiii 사용법이라는 ipynb 파일로 제공할 예정이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum, Kkma, Komoran, Mecab, Twitter\n",
    "\n",
    "def get_tokenizer(token_name):\n",
    "    if token_name == 'Komoran':\n",
    "        tokenizer = Komoran()\n",
    "        \n",
    "    elif token_name == 'Kkma':\n",
    "        tokenizer = Kkma()\n",
    "        \n",
    "    elif token_name == 'Mecab': # window 실행 어려움\n",
    "        tokenizer = Mecab()\n",
    "    \n",
    "    elif token_name == 'Twitter':\n",
    "        tokenizer = Twitter()\n",
    "        \n",
    "    elif token_name == 'Hannanum':\n",
    "        tokenizer = Hannanum()\n",
    "    else:\n",
    "        tokenizer = print('Tokenizer의 명칭을 제대로 확인해 주세요')\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konlpy에서 제공하는 형태소 분석기 활용 함수 \n",
    "\n",
    "#### 공통으로 들어있는 함수 (자주 사용하는 함수만 사용)\n",
    "* morphs / 형태소 분석 후 형태소 전체 추출\n",
    "* nouns / 형태소 분석 후 명사인 형태소만 추출\n",
    "* pos / 형태소 분석 후 품사까지 같이 제공\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕',\n",
       " '하',\n",
       " '세',\n",
       " '요',\n",
       " '.',\n",
       " '저',\n",
       " '는',\n",
       " '지금',\n",
       " '한국어',\n",
       " '임베딩',\n",
       " '스터디',\n",
       " '를',\n",
       " '듣',\n",
       " '고',\n",
       " '있',\n",
       " '습니다',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = get_tokenizer('Hannanum')\n",
    "token.morphs('안녕하세요. 저는 지금 한국어 임베딩 스터디를 듣고 있습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕', '저', '한국어', '임베딩', '스터디']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.nouns('안녕하세요. 저는 지금 한국어 임베딩 스터디를 듣고 있습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('안녕', 'N'),\n",
       " ('하', 'X'),\n",
       " ('세', 'E'),\n",
       " ('요', 'J'),\n",
       " ('.', 'S'),\n",
       " ('저', 'N'),\n",
       " ('는', 'J'),\n",
       " ('지금', 'M'),\n",
       " ('한국어', 'N'),\n",
       " ('임베딩', 'N'),\n",
       " ('스터디', 'N'),\n",
       " ('를', 'J'),\n",
       " ('듣', 'P'),\n",
       " ('고', 'E'),\n",
       " ('있', 'P'),\n",
       " ('습니다', 'E'),\n",
       " ('.', 'S')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.pos('안녕하세요. 저는 지금 한국어 임베딩 스터디를 듣고 있습니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 함수 만들어 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(word, tokenizers, para):\n",
    "\n",
    "    token = get_tokenizer(tokenizers)\n",
    "    \n",
    "    if para == 'morphs' :\n",
    "        return token.morphs(word)\n",
    "    \n",
    "    elif para == 'nouns':\n",
    "        return token.nouns(word)\n",
    "    \n",
    "    elif para == 'pos':\n",
    "        return token.pos(word)\n",
    "    \n",
    "    else:\n",
    "        return print('제대로된 인자를 입력해 주세요')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('안녕', 'NNG'),\n",
       " ('하', 'XSV'),\n",
       " ('세요', 'EFN'),\n",
       " ('.', 'SF'),\n",
       " ('저', 'NP'),\n",
       " ('는', 'JX'),\n",
       " ('지금', 'MAG'),\n",
       " ('한국어', 'NNG'),\n",
       " ('임', 'NNG'),\n",
       " ('베', 'NNG'),\n",
       " ('딩', 'UN'),\n",
       " ('스터디', 'NNG'),\n",
       " ('를', 'JKO'),\n",
       " ('듣', 'VV'),\n",
       " ('고', 'ECE'),\n",
       " ('있', 'VXV'),\n",
       " ('습니다', 'EFN'),\n",
       " ('.', 'SF')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token('안녕하세요. 저는 지금 한국어 임베딩 스터디를 듣고 있습니다.', 'Kkma', 'pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('아버지', 'NNG'),\n",
       " ('가방', 'NNG'),\n",
       " ('에', 'JKM'),\n",
       " ('들어가', 'VV'),\n",
       " ('시', 'EPH'),\n",
       " ('ㄴ다', 'EFN')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token('아버지가방에들어가신다', 'Kkma', 'pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕', '저', '한국어', '임', '임베딩', '베', '딩', '스터디']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token('안녕하세요. 저는 지금 한국어 임베딩 스터디를 듣고 있습니다.', 'Kkma', 'nouns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕',\n",
       " '하',\n",
       " '세요',\n",
       " '.',\n",
       " '저',\n",
       " '는',\n",
       " '지금',\n",
       " '한국어',\n",
       " '임',\n",
       " '베',\n",
       " '딩',\n",
       " '스터디',\n",
       " '를',\n",
       " '듣',\n",
       " '고',\n",
       " '있',\n",
       " '습니다',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token('안녕하세요. 저는 지금 한국어 임베딩 스터디를 듣고 있습니다.', 'Kkma', 'morphs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 위의 내용을 참고하여 문장을 입력했을 때 명사가 아닌 것만 출력되게 하는 코드를 작성해 보세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "def not_N(text,tokenizer):\n",
    "    token = get_tokenizer(tokenizer)\n",
    "    for i,j in token.pos(text) :\n",
    "        if j[0] != 'N':\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가\n",
      "에\n",
      "들\n",
      "어\n",
      "가\n",
      "시ㄴ다\n"
     ]
    }
   ],
   "source": [
    "not_N('아버지가 방에 들어가신다', 'Hannanum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Khaiii 사용하기 \n",
    "\n",
    "제공된 Khaiii활용.ipynb 파일을 Colab에 올려 실행하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford NLP 사용하기\n",
    "\n",
    "Stanford NLP란 딥러닝 기반 형태소 분석 라이브러리이다. \n",
    "\n",
    "스탠포드 대학 연구팀이 개발 \n",
    "\n",
    "의존성 분석 (단어가 특정 단어에 의존되는지에 대한 분석)을 지원한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install pytorch-cpu torchvision-cpu -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanfordnlp\n",
      "  Using cached https://files.pythonhosted.org/packages/41/bf/5d2898febb6e993fcccd90484cba3c46353658511a41430012e901824e94/stanfordnlp-0.2.0-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in c:\\users\\hyoun\\anaconda3\\lib\\site-packages (from stanfordnlp) (1.16.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hyoun\\anaconda3\\lib\\site-packages (from stanfordnlp) (4.36.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\hyoun\\anaconda3\\lib\\site-packages (from stanfordnlp) (3.9.2)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\hyoun\\anaconda3\\lib\\site-packages (from stanfordnlp) (1.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hyoun\\anaconda3\\lib\\site-packages (from stanfordnlp) (2.22.0)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\hyoun\\anaconda3\\lib\\site-packages (from protobuf->stanfordnlp) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hyoun\\anaconda3\\lib\\site-packages (from protobuf->stanfordnlp) (41.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hyoun\\anaconda3\\lib\\site-packages (from requests->stanfordnlp) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\hyoun\\anaconda3\\lib\\site-packages (from requests->stanfordnlp) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\hyoun\\anaconda3\\lib\\site-packages (from requests->stanfordnlp) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\hyoun\\anaconda3\\lib\\site-packages (from requests->stanfordnlp) (1.24.2)\n",
      "Installing collected packages: stanfordnlp\n",
      "Successfully installed stanfordnlp-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: gpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\hyoun\\\\stanfordnlp_resources\\\\ko_gsd_models\\\\ko_gsd_tokenizer.pt', 'lang': 'ko', 'shorthand': 'ko_gsd', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\hyoun\\\\stanfordnlp_resources\\\\ko_gsd_models\\\\ko_gsd_tagger.pt', 'pretrain_path': 'C:\\\\Users\\\\hyoun\\\\stanfordnlp_resources\\\\ko_gsd_models\\\\ko_gsd.pretrain.pt', 'lang': 'ko', 'shorthand': 'ko_gsd', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\hyoun\\\\stanfordnlp_resources\\\\ko_gsd_models\\\\ko_gsd_lemmatizer.pt', 'lang': 'ko', 'shorthand': 'ko_gsd', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\hyoun\\\\stanfordnlp_resources\\\\ko_gsd_models\\\\ko_gsd_parser.pt', 'pretrain_path': 'C:\\\\Users\\\\hyoun\\\\stanfordnlp_resources\\\\ko_gsd_models\\\\ko_gsd.pretrain.pt', 'lang': 'ko', 'shorthand': 'ko_gsd', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "stanfordnlp.download('ko_gsd')\n",
    "\n",
    "nlp = stanfordnlp.Pipeline(lang='ko', treebank='ko_gsd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stanfordnlp.pipeline.doc.Document at 0x1e23b9da8c8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '저는 지금 한국어 임베딩 스터디를 듣고 있습니다.'\n",
    "doc = nlp(text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('저는', '6', 'nsubj')\n",
      "('지금', '6', 'advmod')\n",
      "('한국어', '6', 'obj')\n",
      "('임베딩', '3', 'flat')\n",
      "('스터디를', '3', 'flat')\n",
      "('듣고', '0', 'root')\n",
      "('있습니다', '6', 'flat')\n",
      "('.', '7', 'punct')\n"
     ]
    }
   ],
   "source": [
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Word index=1;text=저는;lemma=저+는;upos=PRON;xpos=VV+ETM;feats=_;governor=6;dependency_relation=nsubj>,\n",
       " <Word index=2;text=지금;lemma=지금;upos=ADV;xpos=MAG;feats=_;governor=6;dependency_relation=advmod>,\n",
       " <Word index=3;text=한국어;lemma=한국어;upos=NOUN;xpos=NNP;feats=_;governor=6;dependency_relation=obj>,\n",
       " <Word index=4;text=임베딩;lemma=임베딩;upos=NOUN;xpos=NNG;feats=_;governor=3;dependency_relation=flat>,\n",
       " <Word index=5;text=스터디를;lemma=스터디+를;upos=NOUN;xpos=NNG+JKO;feats=_;governor=3;dependency_relation=flat>,\n",
       " <Word index=6;text=듣고;lemma=듣+고;upos=VERB;xpos=VV+EC;feats=_;governor=0;dependency_relation=root>,\n",
       " <Word index=7;text=있습니다;lemma=있+습니다;upos=VERB;xpos=VX+EF;feats=_;governor=6;dependency_relation=flat>,\n",
       " <Word index=8;text=.;lemma=.;upos=PUNCT;xpos=SF;feats=_;governor=7;dependency_relation=punct>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = 형태소 lemma 단어의 어원 upos = 품사, governor = 의존단어 위치\n",
    "doc.sentences[0].words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['저', '는']\n",
      "['VV', 'ETM']\n",
      "['지금']\n",
      "['MAG']\n",
      "['한국어']\n",
      "['NNP']\n",
      "['임베딩']\n",
      "['NNG']\n",
      "['스터디', '를']\n",
      "['NNG', 'JKO']\n",
      "['듣', '고']\n",
      "['VV', 'EC']\n",
      "['있', '습니다']\n",
      "['VX', 'EF']\n",
      "['.']\n",
      "['SF']\n"
     ]
    }
   ],
   "source": [
    "for word in doc.sentences[0].words:\n",
    "    print(word.lemma.split('+'))\n",
    "    print(word.xpos.split('+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('저', 'VV'), ('는', 'ETM')]\n",
      "[('지금', 'MAG')]\n",
      "[('한국어', 'NNP')]\n",
      "[('임베딩', 'NNG')]\n",
      "[('스터디', 'NNG'), ('를', 'JKO')]\n",
      "[('듣', 'VV'), ('고', 'EC')]\n",
      "[('있', 'VX'), ('습니다', 'EF')]\n",
      "[('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "for word in doc.sentences[0].words:\n",
    "    print(list(zip(word.lemma.split('+'), word.xpos.split('+'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 수정해서 명사만 출력하는 코드 만들기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명사만 추출하는 함수 만들기\n",
    "def extract_noun(text):\n",
    "    doc = nlp(text)\n",
    "    for size in range(len(doc.sentences)):\n",
    "        for word in doc.sentences[size].words:\n",
    "            for i,k in list(zip(word.lemma.split('+'), word.xpos.split('+'))):\n",
    "                if k.startswith('N'):\n",
    "                    yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아', '더빙', '진짜', '목소리']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(extract_noun(nsmc['document'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아 더빙.. 진짜 짜증나네요 목소리'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsmc['document'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석기 만들기 - 비지도학습\n",
    "\n",
    "### soynlp \n",
    "\n",
    "자세한 설명 https://datascienceschool.net/view-notebook/31eaecec4187428a8dfcab5f686bda8b/ 참조 \n",
    "\n",
    "http://git.ajou.ac.kr/open-source-2018-spring/python_Korean_NLP/tree/master\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting soynlp\n",
      "  Downloading https://files.pythonhosted.org/packages/7e/50/6913dc52a86a6b189419e59f9eef1b8d599cffb6f44f7bb91854165fc603/soynlp-0.0.493-py3-none-any.whl (416kB)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\hyoun\\anaconda3\\lib\\site-packages (from soynlp) (0.21.3)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\hyoun\\anaconda3\\lib\\site-packages (from soynlp) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.12.1 in c:\\users\\hyoun\\anaconda3\\lib\\site-packages (from soynlp) (1.16.5)\n",
      "Requirement already satisfied: psutil>=5.0.1 in c:\\users\\hyoun\\anaconda3\\lib\\site-packages (from soynlp) (5.6.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\hyoun\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->soynlp) (0.13.2)\n",
      "Installing collected packages: soynlp\n",
      "Successfully installed soynlp-0.0.493\n"
     ]
    }
   ],
   "source": [
    "!pip install soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30091"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp import DoublespaceLineCorpus\n",
    "\n",
    "# 문서 단위 말뭉치 생성 \n",
    "corpus = DoublespaceLineCorpus(\"2016-10-20.txt\")\n",
    "len(corpus)  # 문서의 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "1 19  1990  52 1 22\n",
      "2 오패산터널 총격전 용의자 검거 서울 연합뉴스 경찰 관계자들이 19일 오후 서울 강북구 오패산 터널 인근에서 사제 총기를 발사해 경찰을 살해한 용의자 성모씨를 검거하고 있다 성씨는 검거 당시 서바이벌 게임에서 쓰는 방탄조끼에 헬멧까지 착용한 상태였다 독자제공 영상 캡처 연합뉴스  서울 연합뉴스 김은경 기자 사제 총기로 경찰을 살해한 범인 성모 46 씨는 주도면밀했다  경찰에 따르면 성씨는 19일 오후 강북경찰서 인근 부동산 업소 밖에서 부동산업자 이모 67 씨가 나오기를 기다렸다 이씨와는 평소에도 말다툼을 자주 한 것으로 알려졌다  이씨가 나와 걷기 시작하자 성씨는 따라가면서 미리 준비해온 사제 총기를 이씨에게 발사했다 총알이 빗나가면서 이씨는 도망갔다 그 빗나간 총알은 지나가던 행인 71 씨의 배를 스쳤다  성씨는 강북서 인근 치킨집까지 이씨 뒤를 쫓으며 실랑이하다 쓰러뜨린 후 총기와 함께 가져온 망치로 이씨 머리를 때렸다  이 과정에서 오후 6시 20분께 강북구 번동 길 위에서 사람들이 싸우고 있다 총소리가 났다 는 등의 신고가 여러건 들어왔다  5분 후에 성씨의 전자발찌가 훼손됐다는 신고가 보호관찰소 시스템을 통해 들어왔다 성범죄자로 전자발찌를 차고 있던 성씨는 부엌칼로 직접 자신의 발찌를 끊었다  용의자 소지 사제총기 2정 서울 연합뉴스 임헌정 기자 서울 시내에서 폭행 용의자가 현장 조사를 벌이던 경찰관에게 사제총기를 발사해 경찰관이 숨졌다 19일 오후 6시28분 강북구 번동에서 둔기로 맞았다 는 폭행 피해 신고가 접수돼 현장에서 조사하던 강북경찰서 번동파출소 소속 김모 54 경위가 폭행 용의자 성모 45 씨가 쏜 사제총기에 맞고 쓰러진 뒤 병원에 옮겨졌으나 숨졌다 사진은 용의자가 소지한 사제총기  신고를 받고 번동파출소에서 김창호 54 경위 등 경찰들이 오후 6시 29분께 현장으로 출동했다 성씨는 그사이 부동산 앞에 놓아뒀던 가방을 챙겨 오패산 쪽으로 도망간 후였다  김 경위는 오패산 터널 입구 오른쪽의 급경사에서 성씨에게 접근하다가 오후 6시 33분께 풀숲에 숨은 성씨가 허공에 난사한 10여발의 총알 중 일부를 왼쪽 어깨 뒷부분에 맞고 쓰러졌다  김 경위는 구급차가 도착했을 때 이미 의식이 없었고 심폐소생술을 하며 병원으로 옮겨졌으나 총알이 폐를 훼손해 오후 7시 40분께 사망했다  김 경위는 외근용 조끼를 입고 있었으나 총알을 막기에는 역부족이었다  머리에 부상을 입은 이씨도 함께 병원으로 이송됐으나 생명에는 지장이 없는 것으로 알려졌다  성씨는 오패산 터널 밑쪽 숲에서 오후 6시 45분께 잡혔다  총격현장 수색하는 경찰들 서울 연합뉴스 이효석 기자 19일 오후 서울 강북구 오패산 터널 인근에서 경찰들이 폭행 용의자가 사제총기를 발사해 경찰관이 사망한 사건을 조사 하고 있다  총 때문에 쫓던 경관들과 민간인들이 몸을 숨겼는데 인근 신발가게 직원 이모씨가 다가가 성씨를 덮쳤고 이어 현장에 있던 다른 상인들과 경찰이 가세해 체포했다  성씨는 경찰에 붙잡힌 직후 나 자살하려고 한 거다 맞아 죽어도 괜찮다 고 말한 것으로 전해졌다  성씨 자신도 경찰이 발사한 공포탄 1발 실탄 3발 중 실탄 1발을 배에 맞았으나 방탄조끼를 입은 상태여서 부상하지는 않았다  경찰은 인근을 수색해 성씨가 만든 사제총 16정과 칼 7개를 압수했다 실제 폭발할지는 알 수 없는 요구르트병에 무언가를 채워두고 심지를 꽂은 사제 폭탄도 발견됐다  일부는 숲에서 발견됐고 일부는 성씨가 소지한 가방 안에 있었다\n",
      "3 테헤란 연합뉴스 강훈상 특파원 이용 승객수 기준 세계 최대 공항인 아랍에미리트 두바이국제공항은 19일 현지시간 이 공항을 이륙하는 모든 항공기의 탑승객은 삼성전자의 갤럭시노트7을 휴대하면 안 된다고 밝혔다  두바이국제공항은 여러 항공 관련 기구의 권고에 따라 안전성에 우려가 있는 스마트폰 갤럭시노트7을 휴대하고 비행기를 타면 안 된다 며 탑승 전 검색 중 발견되면 압수할 계획 이라고 발표했다  공항 측은 갤럭시노트7의 배터리가 폭발 우려가 제기된 만큼 이 제품을 갖고 공항 안으로 들어오지 말라고 이용객에 당부했다  이런 조치는 두바이국제공항 뿐 아니라 신공항인 두바이월드센터에도 적용된다  배터리 폭발문제로 회수된 갤럭시노트7 연합뉴스자료사진\n",
      "4 브뤼셀 연합뉴스 김병수 특파원 독일 정부는 19일 원자력발전소를 폐쇄하기로 함에 따라 원자력 발전소 운영자들에게 핵폐기물 처리를 지원하는 펀드에 235억 유로 260억 달러 29조 원 를 지불하도록 하는 계획을 승인했다고 언론들이 보도했다  앞서 독일은 5년 전 일본 후쿠시마 원전사태 이후 오는 2022년까지 원전 17기를 모두 폐쇄하기로 하고 오는 2050년까지 전기생산량의 80 를 재생에너지로 충당하는 것을 목표로 세웠다  이날 내각을 통과한 법안은 원전 운영자들이 원전 해체와 핵폐기물 처리를 위한 포장을 책임지고 정부는 핵폐기물 보관을 책임지도록 했다  독일 경제부는 전력회사들과 공식적인 접촉은 아직 합의되지 않았다고 밝혔다  독일 원자력 발전소 연합뉴스 자료사진\n"
     ]
    }
   ],
   "source": [
    "# 앞 5개의 문서 인쇄\n",
    "i = 0\n",
    "for d in corpus:\n",
    "    print(i, d)\n",
    "    i += 1\n",
    "    if i > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223357"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 단위 말뭉치 생성 \n",
    "corpus = DoublespaceLineCorpus(\"2016-10-20.txt\", iter_sent=True)\n",
    "len(corpus)  # 문장의 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 19\n",
      "1 1990\n",
      "2 52 1 22\n",
      "3 오패산터널 총격전 용의자 검거 서울 연합뉴스 경찰 관계자들이 19일 오후 서울 강북구 오패산 터널 인근에서 사제 총기를 발사해 경찰을 살해한 용의자 성모씨를 검거하고 있다 성씨는 검거 당시 서바이벌 게임에서 쓰는 방탄조끼에 헬멧까지 착용한 상태였다 독자제공 영상 캡처 연합뉴스\n",
      "4 서울 연합뉴스 김은경 기자 사제 총기로 경찰을 살해한 범인 성모 46 씨는 주도면밀했다\n"
     ]
    }
   ],
   "source": [
    "# 앞 5개의 문장 인쇄\n",
    "i = 0\n",
    "for d in corpus:\n",
    "    print(i, d)\n",
    "    i += 1\n",
    "    if i > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 0.734 Gbse memory 0.801 Gb\n",
      "Wall time: 46.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from soynlp.word import WordExtractor\n",
    "\n",
    "word_extractor = WordExtractor()\n",
    "word_extractor.train(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all cohesion probabilities was computed. # words = 223348\n",
      "all branching entropies was computed # words = 360721\n",
      "all accessor variety was computed # words = 360721\n"
     ]
    }
   ],
   "source": [
    "word_score = word_extractor.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohesion\n",
    "\n",
    "![title](cohesion.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1943363253634125"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score[\"연합\"].cohesion_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43154839105434084"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score[\"연합뉴\"].cohesion_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5710254410737682"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score[\"연합뉴스\"].cohesion_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1535595043355021"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score[\"연합뉴스는\"].cohesion_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](branching.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42721236711742844"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score[\"연합\"].right_branching_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score[\"연합뉴\"].right_branching_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](L_token.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('아버지가', '방에들어가신다.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "scores = {word:score.cohesion_forward for word, score in word_score.items()}\n",
    "l_tokenizer = LTokenizer(scores=scores)\n",
    "\n",
    "l_tokenizer.tokenize(\"아버지가방에들어가신다.\", flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('아버지가', ''), ('방에', ''), ('들어', '가신다.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_tokenizer.tokenize(\"아버지가 방에 들어가신다.\", flatten=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](max_token.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아버지가', '방에', '들어', '가신다.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "\n",
    "maxscore_tokenizer = MaxScoreTokenizer(scores=scores)\n",
    "maxscore_tokenizer.tokenize(\"아버지가방에들어가신다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아버지가', '방에', '들어', '가신다.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxscore_tokenizer.tokenize(\"아버지가 방에 들어가신다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Piece\n",
    "\n",
    "구글에서 BPE 알고리즘과 Unigram Language Model Tokenizer를 구현한 모델\n",
    "\n",
    "BPE란 무엇인지에 대한 자료 https://wikidocs.net/22592\n",
    "\n",
    "* BPE는 글자 단위에서 점차적으로 단어 집합을 만들어 내는 방식 \n",
    "\n",
    "* 훈련 데이터 내에 있는 모든 글자 또는 유니코드 단위로 집합 만들고 여러 유니그램 하나로 통합 - 즉 패턴을 통해 해당 단어에 대하여 찾아낸다 생각하면 된다.\n",
    "\n",
    "* 실제로 이번 고어번역기 모델에 적용한 결과 전과 대비해 엄청나게 좋은 성능을 보여주었음 단 이때 데이터에 대하여 충분한 전처리와 추출작업 등을 먼저 진행하여야 했다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용법 \n",
    "\n",
    "1. 토큰화를 진행하고자 하는 데이터에 대한 파일을 만든다. \n",
    "\n",
    "2. SentencePieceTrainer의 train 기능을 활용 자신이 원하는 oov 및 pad, unk 등 옵션을 설정하고 토큰을 몇개로 설정할 것인지, 만들어진 모델의 이름을 어떻게 저장할 것인지를 설정한다. \n",
    "\n",
    "3. 만들어진 모델을 불러온다. \n",
    "\n",
    "4. EncodeAsPieces 함수를 활용 토큰화를 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 댓글 자료 만들기.\n",
    "with open('naver.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(nsmc['document']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# txt 파일을 불러와 token 만드는 작업 시행, -1은 사용하지 않겠다는 의미 \n",
    "# 단어의 개수는 5000개를 사용할 것임 \n",
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.Train('--pad_id=0 --bos_id=-1 --eos_id=-1 --unk_id=1 --input=naver.txt --model_prefix=naver --vocab_size=5000 --model_type=unigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<unk>': 1,\n",
       " '▁': 2,\n",
       " '.': 3,\n",
       " '이': 4,\n",
       " '..': 5,\n",
       " '가': 6,\n",
       " '▁영화': 7,\n",
       " '의': 8,\n",
       " '...': 9,\n",
       " '도': 10,\n",
       " '는': 11,\n",
       " '을': 12,\n",
       " '고': 13,\n",
       " '에': 14,\n",
       " ',': 15,\n",
       " '다': 16,\n",
       " '은': 17,\n",
       " '지': 18,\n",
       " '한': 19,\n",
       " '?': 20,\n",
       " '만': 21,\n",
       " '로': 22,\n",
       " '를': 23,\n",
       " '게': 24,\n",
       " '나': 25,\n",
       " '▁너무': 26,\n",
       " '영화': 27,\n",
       " '!': 28,\n",
       " '▁이': 29,\n",
       " '▁정말': 30,\n",
       " '리': 31,\n",
       " '어': 32,\n",
       " '아': 33,\n",
       " '기': 34,\n",
       " '인': 35,\n",
       " '과': 36,\n",
       " '하고': 37,\n",
       " '라': 38,\n",
       " '▁진짜': 39,\n",
       " '~': 40,\n",
       " '서': 41,\n",
       " '네': 42,\n",
       " '해': 43,\n",
       " '으로': 44,\n",
       " '점': 45,\n",
       " '요': 46,\n",
       " '자': 47,\n",
       " '▁안': 48,\n",
       " '▁그': 49,\n",
       " '에서': 50,\n",
       " '스': 51,\n",
       " '들': 52,\n",
       " '네요': 53,\n",
       " '와': 54,\n",
       " '▁아': 55,\n",
       " '▁잘': 56,\n",
       " '수': 57,\n",
       " '음': 58,\n",
       " '면': 59,\n",
       " '하는': 60,\n",
       " '거': 61,\n",
       " '▁왜': 62,\n",
       " '▁수': 63,\n",
       " '▁다': 64,\n",
       " '사': 65,\n",
       " '....': 66,\n",
       " '▁1': 67,\n",
       " '하': 68,\n",
       " '!!': 69,\n",
       " '▁이런': 70,\n",
       " '시': 71,\n",
       " '▁더': 72,\n",
       " '니': 73,\n",
       " '대': 74,\n",
       " '▁본': 75,\n",
       " '함': 76,\n",
       " '지만': 77,\n",
       " '야': 78,\n",
       " '하다': 79,\n",
       " '하게': 80,\n",
       " '보': 81,\n",
       " '일': 82,\n",
       " '▁그냥': 83,\n",
       " '우': 84,\n",
       " '▁드라마': 85,\n",
       " '진': 86,\n",
       " '이다': 87,\n",
       " '주': 88,\n",
       " '정': 89,\n",
       " '여': 90,\n",
       " '드': 91,\n",
       " '할': 92,\n",
       " '는데': 93,\n",
       " '▁내': 94,\n",
       " '미': 95,\n",
       " '임': 96,\n",
       " '성': 97,\n",
       " '▁한': 98,\n",
       " '▁10': 99,\n",
       " '구': 100,\n",
       " '데': 101,\n",
       " '상': 102,\n",
       " '▁보고': 103,\n",
       " '신': 104,\n",
       " '▁못': 105,\n",
       " '▁연기': 106,\n",
       " '▁보': 107,\n",
       " '▁평점': 108,\n",
       " '던': 109,\n",
       " '안': 110,\n",
       " '려': 111,\n",
       " '▁나': 112,\n",
       " '까지': 113,\n",
       " '▁최고': 114,\n",
       " '▁말': 115,\n",
       " '장': 116,\n",
       " '부': 117,\n",
       " '해서': 118,\n",
       " '▁영화를': 119,\n",
       " '▁좀': 120,\n",
       " '냐': 121,\n",
       " '그': 122,\n",
       " '마': 123,\n",
       " '들이': 124,\n",
       " '치': 125,\n",
       " '보다': 126,\n",
       " '▁개': 127,\n",
       " '러': 128,\n",
       " '적': 129,\n",
       " '▁쓰레기': 130,\n",
       " '중': 131,\n",
       " '듯': 132,\n",
       " '트': 133,\n",
       " '▁스토리': 134,\n",
       " '화': 135,\n",
       " '▁참': 136,\n",
       " '때': 137,\n",
       " '건': 138,\n",
       " '오': 139,\n",
       " '▁이거': 140,\n",
       " '▁봤는데': 141,\n",
       " '▁ᄏᄏ': 142,\n",
       " '▁최고의': 143,\n",
       " '분': 144,\n",
       " '▁없는': 145,\n",
       " '▁내가': 146,\n",
       " '습니다': 147,\n",
       " '▁다시': 148,\n",
       " '원': 149,\n",
       " '히': 150,\n",
       " '물': 151,\n",
       " '▁없다': 152,\n",
       " '소': 153,\n",
       " '전': 154,\n",
       " '▁2': 155,\n",
       " '▁뭐': 156,\n",
       " '▁있는': 157,\n",
       " '▁생각': 158,\n",
       " '▁영화는': 159,\n",
       " '!!!': 160,\n",
       " '▁완전': 161,\n",
       " '▁영화가': 162,\n",
       " '▁이건': 163,\n",
       " '식': 164,\n",
       " '보고': 165,\n",
       " '비': 166,\n",
       " 'ᄏᄏ': 167,\n",
       " '▁무': 168,\n",
       " '개': 169,\n",
       " '질': 170,\n",
       " '▁보는': 171,\n",
       " '랑': 172,\n",
       " '▁것': 173,\n",
       " '▁난': 174,\n",
       " '1': 175,\n",
       " '▁좋은': 176,\n",
       " '것': 177,\n",
       " '▁하': 178,\n",
       " '^^': 179,\n",
       " '어요': 180,\n",
       " '▁오': 181,\n",
       " '▁이렇게': 182,\n",
       " '내': 183,\n",
       " '▁전': 184,\n",
       " '세': 185,\n",
       " '▁이게': 186,\n",
       " '년': 187,\n",
       " '▁3': 188,\n",
       " '편': 189,\n",
       " '적인': 190,\n",
       " '입니다': 191,\n",
       " '라고': 192,\n",
       " '감': 193,\n",
       " '▁볼': 194,\n",
       " '▁작품': 195,\n",
       " '본': 196,\n",
       " '▁지': 197,\n",
       " '제': 198,\n",
       " '워': 199,\n",
       " '▁넘': 200,\n",
       " '긴': 201,\n",
       " '용': 202,\n",
       " '했다': 203,\n",
       " '유': 204,\n",
       " '▁어': 205,\n",
       " '작': 206,\n",
       " 'ᄏ': 207,\n",
       " '걸': 208,\n",
       " '▁조': 209,\n",
       " '~~': 210,\n",
       " '▁저': 211,\n",
       " '무': 212,\n",
       " '명': 213,\n",
       " ';;': 214,\n",
       " '인데': 215,\n",
       " '▁주': 216,\n",
       " '▁사': 217,\n",
       " '으면': 218,\n",
       " '▁별로': 219,\n",
       " '▁정': 220,\n",
       " '선': 221,\n",
       " '같은': 222,\n",
       " '▁시간': 223,\n",
       " '▁재밌게': 224,\n",
       " '영': 225,\n",
       " '▁평점이': 226,\n",
       " '▁많이': 227,\n",
       " '▁사랑': 228,\n",
       " '▁재미': 229,\n",
       " '▁마지막': 230,\n",
       " '운': 231,\n",
       " '말': 232,\n",
       " '▁유': 233,\n",
       " '연': 234,\n",
       " '부터': 235,\n",
       " '조': 236,\n",
       " '▁마': 237,\n",
       " '▁또': 238,\n",
       " '남': 239,\n",
       " '래': 240,\n",
       " '동': 241,\n",
       " '간': 242,\n",
       " '된': 243,\n",
       " '줄': 244,\n",
       " '▁김': 245,\n",
       " '타': 246,\n",
       " '더': 247,\n",
       " '▁역시': 248,\n",
       " '▁감동': 249,\n",
       " '▁그리고': 250,\n",
       " '▁대': 251,\n",
       " '▁기대': 252,\n",
       " '하지': 253,\n",
       " '당': 254,\n",
       " '라는': 255,\n",
       " '급': 256,\n",
       " '▁아니': 257,\n",
       " '없는': 258,\n",
       " '▁중': 259,\n",
       " '울': 260,\n",
       " '2': 261,\n",
       " '▁하나': 262,\n",
       " '이나': 263,\n",
       " '▁기': 264,\n",
       " '▁일': 265,\n",
       " '란': 266,\n",
       " '군': 267,\n",
       " '▁자': 268,\n",
       " '▁꼭': 269,\n",
       " '진짜': 270,\n",
       " '린': 271,\n",
       " '발': 272,\n",
       " '디': 273,\n",
       " '씨': 274,\n",
       " '▁하는': 275,\n",
       " '▁비': 276,\n",
       " '??': 277,\n",
       " '▁감독': 278,\n",
       " '바': 279,\n",
       " '▁배우': 280,\n",
       " '심': 281,\n",
       " \"'\": 282,\n",
       " '레': 283,\n",
       " '▁지금': 284,\n",
       " '르': 285,\n",
       " '▁액션': 286,\n",
       " '▁별': 287,\n",
       " '호': 288,\n",
       " '▁소': 289,\n",
       " '▁거': 290,\n",
       " '연기': 291,\n",
       " '▁남': 292,\n",
       " '▁만든': 293,\n",
       " ')': 294,\n",
       " '▁부': 295,\n",
       " '방': 296,\n",
       " '위': 297,\n",
       " '▁명작': 298,\n",
       " '▁한국': 299,\n",
       " '▁미': 300,\n",
       " '루': 301,\n",
       " '▁무슨': 302,\n",
       " '너무': 303,\n",
       " '난': 304,\n",
       " '▁아깝다': 305,\n",
       " '▁처음': 306,\n",
       " '▁이야기': 307,\n",
       " '에게': 308,\n",
       " '▁가장': 309,\n",
       " '▁ᄏᄏᄏ': 310,\n",
       " '▁돈': 311,\n",
       " '살': 312,\n",
       " '드라마': 313,\n",
       " '▁연출': 314,\n",
       " '▁없고': 315,\n",
       " '▁모': 316,\n",
       " '▁내용': 317,\n",
       " '▁두': 318,\n",
       " '▁신': 319,\n",
       " '▁없': 320,\n",
       " '좀': 321,\n",
       " 'ᄏᄏᄏ': 322,\n",
       " '▁다른': 323,\n",
       " '있는': 324,\n",
       " '▁느낌': 325,\n",
       " '모': 326,\n",
       " '까': 327,\n",
       " '프': 328,\n",
       " '후': 329,\n",
       " '저': 330,\n",
       " '판': 331,\n",
       " '국': 332,\n",
       " '있': 333,\n",
       " '준': 334,\n",
       " '생': 335,\n",
       " '엔': 336,\n",
       " '▁보면': 337,\n",
       " '3': 338,\n",
       " '류': 339,\n",
       " ';': 340,\n",
       " '▁사람': 341,\n",
       " 'ᅲᅲ': 342,\n",
       " '▁좋아': 343,\n",
       " '노': 344,\n",
       " '인지': 345,\n",
       " '크': 346,\n",
       " '▁할': 347,\n",
       " '못': 348,\n",
       " '합니다': 349,\n",
       " '경': 350,\n",
       " 'e': 351,\n",
       " '었다': 352,\n",
       " '민': 353,\n",
       " '한다': 354,\n",
       " '하나': 355,\n",
       " '님': 356,\n",
       " '처럼': 357,\n",
       " '번': 358,\n",
       " '▁일본': 359,\n",
       " '들의': 360,\n",
       " '시간': 361,\n",
       " '길': 362,\n",
       " '져': 363,\n",
       " '단': 364,\n",
       " '▁인생': 365,\n",
       " 'ᅲ': 366,\n",
       " '터': 367,\n",
       " '두': 368,\n",
       " '▁제': 369,\n",
       " '▁애': 370,\n",
       " '든': 371,\n",
       " '관': 372,\n",
       " '피': 373,\n",
       " '▁반': 374,\n",
       " '애': 375,\n",
       " '▁우리': 376,\n",
       " '되': 377,\n",
       " '희': 378,\n",
       " '했던': 379,\n",
       " '▁불': 380,\n",
       " '▁때': 381,\n",
       " '니까': 382,\n",
       " '재': 383,\n",
       " '볼': 384,\n",
       " '▁되': 385,\n",
       " '▁ᄏ': 386,\n",
       " '되는': 387,\n",
       " '▁시': 388,\n",
       " '정말': 389,\n",
       " '▁있': 390,\n",
       " '문': 391,\n",
       " '죠': 392,\n",
       " '▁재': 393,\n",
       " '▁구': 394,\n",
       " '▁많은': 395,\n",
       " '으': 396,\n",
       " '5': 397,\n",
       " '버': 398,\n",
       " '체': 399,\n",
       " '면서': 400,\n",
       " '▁짱': 401,\n",
       " 'o': 402,\n",
       " '▁ᅲᅲ': 403,\n",
       " '파': 404,\n",
       " '▁있다': 405,\n",
       " '▁말이': 406,\n",
       " '▁강': 407,\n",
       " '▁여자': 408,\n",
       " '었': 409,\n",
       " '잘': 410,\n",
       " '겨': 411,\n",
       " '행': 412,\n",
       " '▁남자': 413,\n",
       " '▁이상': 414,\n",
       " 'a': 415,\n",
       " '▁알': 416,\n",
       " '▁여': 417,\n",
       " '▁만들어': 418,\n",
       " '겠다': 419,\n",
       " '-': 420,\n",
       " '▁끝까지': 421,\n",
       " '▁재밌다': 422,\n",
       " '날': 423,\n",
       " '이라': 424,\n",
       " '니다': 425,\n",
       " '티': 426,\n",
       " '▁같은': 427,\n",
       " '▁해': 428,\n",
       " '차': 429,\n",
       " '▁우': 430,\n",
       " '역': 431,\n",
       " \"▁'\": 432,\n",
       " '▁장': 433,\n",
       " '▁좋다': 434,\n",
       " '구나': 435,\n",
       " '즈': 436,\n",
       " '하지만': 437,\n",
       " '▁영화다': 438,\n",
       " '(': 439,\n",
       " '회': 440,\n",
       " '▁배우들': 441,\n",
       " '▁후': 442,\n",
       " '▁봤다': 443,\n",
       " '카': 444,\n",
       " '림': 445,\n",
       " '▁ᅳᅳ': 446,\n",
       " '며': 447,\n",
       " '▁전혀': 448,\n",
       " '▁솔직히': 449,\n",
       " '인가': 450,\n",
       " '없이': 451,\n",
       " '보는': 452,\n",
       " '▁바': 453,\n",
       " '예': 454,\n",
       " '들은': 455,\n",
       " '없': 456,\n",
       " '하기': 457,\n",
       " '박': 458,\n",
       " '▁뭔가': 459,\n",
       " '▁울': 460,\n",
       " '▁모두': 461,\n",
       " '실': 462,\n",
       " '름': 463,\n",
       " '현': 464,\n",
       " '만큼': 465,\n",
       " '▁보기': 466,\n",
       " '쳐': 467,\n",
       " '였': 468,\n",
       " '▁한번': 469,\n",
       " '▁봐도': 470,\n",
       " '하면': 471,\n",
       " '▁모든': 472,\n",
       " '▁줄': 473,\n",
       " '▁살': 474,\n",
       " '키': 475,\n",
       " '였다': 476,\n",
       " '▁하지만': 477,\n",
       " '▁대한': 478,\n",
       " '▁같다': 479,\n",
       " '반': 480,\n",
       " '▁좋': 481,\n",
       " '▁박': 482,\n",
       " '▁아주': 483,\n",
       " '▁와': 484,\n",
       " '다가': 485,\n",
       " '형': 486,\n",
       " 'ᅳᅳ': 487,\n",
       " '이랑': 488,\n",
       " 'd': 489,\n",
       " '▁공': 490,\n",
       " '▁아니라': 491,\n",
       " '▁원작': 492,\n",
       " '▁만들': 493,\n",
       " '▁눈': 494,\n",
       " '▁이해': 495,\n",
       " '▁최악': 496,\n",
       " '▁ᄒᄒ': 497,\n",
       " '달': 498,\n",
       " '망': 499,\n",
       " '▁코미디': 500,\n",
       " '다니': 501,\n",
       " '▁동': 502,\n",
       " '▁스': 503,\n",
       " '▁끝': 504,\n",
       " '▁굿': 505,\n",
       " '적으로': 506,\n",
       " '▁현실': 507,\n",
       " '때문에': 508,\n",
       " '▁대박': 509,\n",
       " '어서': 510,\n",
       " '지는': 511,\n",
       " '▁욕': 512,\n",
       " '▁그런': 513,\n",
       " '지도': 514,\n",
       " '▁재밌': 515,\n",
       " '석': 516,\n",
       " '는게': 517,\n",
       " '▁반전': 518,\n",
       " '하네요': 519,\n",
       " '▁최악의': 520,\n",
       " '▁전개': 521,\n",
       " '학': 522,\n",
       " '없다': 523,\n",
       " '산': 524,\n",
       " '속': 525,\n",
       " '금': 526,\n",
       " '▁배': 527,\n",
       " '▁나오': 528,\n",
       " '▁심': 529,\n",
       " ',,': 530,\n",
       " '▁나오는': 531,\n",
       " '▁매': 532,\n",
       " '▁주인공': 533,\n",
       " '▁재미있게': 534,\n",
       " '녀': 535,\n",
       " '▁가족': 536,\n",
       " '▁이영화': 537,\n",
       " '▁어떻게': 538,\n",
       " '생각': 539,\n",
       " '▁실망': 540,\n",
       " '승': 541,\n",
       " '▁이걸': 542,\n",
       " '▁내내': 543,\n",
       " '봤는데': 544,\n",
       " '▁망': 545,\n",
       " '▁공포': 546,\n",
       " '▁장면': 547,\n",
       " '▁계속': 548,\n",
       " '극': 549,\n",
       " '양': 550,\n",
       " '▁파': 551,\n",
       " '코': 552,\n",
       " '▁가슴': 553,\n",
       " '것도': 554,\n",
       " '▁아닌': 555,\n",
       " '배우': 556,\n",
       " '색': 557,\n",
       " '▁재미없다': 558,\n",
       " '▁피': 559,\n",
       " '▁않고': 560,\n",
       " '▁내용이': 561,\n",
       " '스토리': 562,\n",
       " '토': 563,\n",
       " '▁결말': 564,\n",
       " '▁속': 565,\n",
       " 'ᄒᄒ': 566,\n",
       " '▁맞': 567,\n",
       " '▁개봉': 568,\n",
       " '뿐': 569,\n",
       " '이라는': 570,\n",
       " '▁4': 571,\n",
       " '태': 572,\n",
       " '을까': 573,\n",
       " '▁대사': 574,\n",
       " '력': 575,\n",
       " '점이': 576,\n",
       " '맨': 577,\n",
       " '건지': 578,\n",
       " '보면': 579,\n",
       " '▁제목': 580,\n",
       " '한테': 581,\n",
       " '▁인': 582,\n",
       " '려고': 583,\n",
       " '▁쓰': 584,\n",
       " '▁정도': 585,\n",
       " '배': 586,\n",
       " '10': 587,\n",
       " '향': 588,\n",
       " '점도': 589,\n",
       " '았': 590,\n",
       " '하네': 591,\n",
       " 't': 592,\n",
       " '▁7': 593,\n",
       " '매': 594,\n",
       " '사람': 595,\n",
       " '▁딱': 596,\n",
       " '넘': 597,\n",
       " '4': 598,\n",
       " '▁죽': 599,\n",
       " '▁예': 600,\n",
       " '▁영': 601,\n",
       " '▁않은': 602,\n",
       " '▁그래도': 603,\n",
       " '집': 604,\n",
       " '▁수준': 605,\n",
       " '▁않': 606,\n",
       " '스트': 607,\n",
       " '등': 608,\n",
       " '▁기억': 609,\n",
       " '포': 610,\n",
       " '장면': 611,\n",
       " '▁인간': 612,\n",
       " '▁추천': 613,\n",
       " '▁조금': 614,\n",
       " '▁낮': 615,\n",
       " '럽': 616,\n",
       " '슬': 617,\n",
       " '공': 618,\n",
       " '▁9': 619,\n",
       " '▁스릴러': 620,\n",
       " '계': 621,\n",
       " '▁20': 622,\n",
       " '담': 623,\n",
       " '▁최': 624,\n",
       " '▁공감': 625,\n",
       " '▁연기력': 626,\n",
       " '▁아니다': 627,\n",
       " '▁노': 628,\n",
       " '이고': 629,\n",
       " '▁제대로': 630,\n",
       " '했는데': 631,\n",
       " '결': 632,\n",
       " '▁긴장감': 633,\n",
       " '알': 634,\n",
       " '능': 635,\n",
       " '절': 636,\n",
       " '▁5': 637,\n",
       " '▁연기가': 638,\n",
       " '죽': 639,\n",
       " '▁연': 640,\n",
       " '왜': 641,\n",
       " '▁캐릭터': 642,\n",
       " '리는': 643,\n",
       " '▁초': 644,\n",
       " '▁건': 645,\n",
       " '▁아름다운': 646,\n",
       " '평': 647,\n",
       " '▁싶': 648,\n",
       " '인듯': 649,\n",
       " '해요': 650,\n",
       " '격': 651,\n",
       " '▁제일': 652,\n",
       " '는거': 653,\n",
       " '입': 654,\n",
       " '♥': 655,\n",
       " '▁요즘': 656,\n",
       " '릴': 657,\n",
       " '케': 658,\n",
       " '움': 659,\n",
       " '▁평': 660,\n",
       " '▁영상': 661,\n",
       " '▁만드는': 662,\n",
       " '씬': 663,\n",
       " '더라': 664,\n",
       " '▁발': 665,\n",
       " '▁원': 666,\n",
       " '!!!!': 667,\n",
       " '▁성': 668,\n",
       " '▁차': 669,\n",
       " '▁진': 670,\n",
       " '▁8': 671,\n",
       " '업': 672,\n",
       " '▁근데': 673,\n",
       " '~~~': 674,\n",
       " '빠': 675,\n",
       " '▁이제': 676,\n",
       " '밖에': 677,\n",
       " '▁듯': 678,\n",
       " '이란': 679,\n",
       " '잇': 680,\n",
       " '았다': 681,\n",
       " '▁연기도': 682,\n",
       " '▁감독이': 683,\n",
       " '쓰': 684,\n",
       " '▁표현': 685,\n",
       " '따': 686,\n",
       " '▁아까운': 687,\n",
       " '▁절대': 688,\n",
       " '봐도': 689,\n",
       " '대로': 690,\n",
       " '▁않는': 691,\n",
       " 's': 692,\n",
       " '정도': 693,\n",
       " '▁소재': 694,\n",
       " '▁미국': 695,\n",
       " '▁노래': 696,\n",
       " '▁웃': 697,\n",
       " '▁음악': 698,\n",
       " '▁싶다': 699,\n",
       " '종': 700,\n",
       " '난다': 701,\n",
       " '년대': 702,\n",
       " '▁상': 703,\n",
       " '꺼': 704,\n",
       " '▁재미없': 705,\n",
       " '론': 706,\n",
       " '기도': 707,\n",
       " '했': 708,\n",
       " '▁내용도': 709,\n",
       " '아니': 710,\n",
       " '▁보다가': 711,\n",
       " '통': 712,\n",
       " '▁보다': 713,\n",
       " '▁OO': 714,\n",
       " '▁스토리가': 715,\n",
       " 'ᄒ': 716,\n",
       " '▁실': 717,\n",
       " '▁선': 718,\n",
       " '▁괜찮': 719,\n",
       " 'ᄏᄏᄏᄏ': 720,\n",
       " '▁졸': 721,\n",
       " '▁아이': 722,\n",
       " '8': 723,\n",
       " '내용': 724,\n",
       " '7': 725,\n",
       " '▁시작': 726,\n",
       " '▁세': 727,\n",
       " '들을': 728,\n",
       " '▁억지': 729,\n",
       " '테': 730,\n",
       " '참': 731,\n",
       " '뭐': 732,\n",
       " '근': 733,\n",
       " '▁재밋': 734,\n",
       " 'c': 735,\n",
       " '▁나는': 736,\n",
       " '감동': 737,\n",
       " '▁아니고': 738,\n",
       " '▁0': 739,\n",
       " '었는데': 740,\n",
       " '▁애니': 741,\n",
       " '▁엄청': 742,\n",
       " '감독': 743,\n",
       " '▁눈물': 744,\n",
       " 'ᅮ': 745,\n",
       " '언': 746,\n",
       " '▁막': 747,\n",
       " '▁막장': 748,\n",
       " '먹': 749,\n",
       " '않': 750,\n",
       " '증': 751,\n",
       " '짱': 752,\n",
       " '는지': 753,\n",
       " '▁위': 754,\n",
       " '▁작가': 755,\n",
       " '▁단': 756,\n",
       " '▁잠': 757,\n",
       " '불': 758,\n",
       " '▁허': 759,\n",
       " '▁극': 760,\n",
       " '0': 761,\n",
       " 'i': 762,\n",
       " '▁사람들': 763,\n",
       " '돈': 764,\n",
       " '하면서': 765,\n",
       " '적이': 766,\n",
       " '라도': 767,\n",
       " '이라고': 768,\n",
       " '▁남는': 769,\n",
       " '뻔': 770,\n",
       " '▁걸': 771,\n",
       " '▁의미': 772,\n",
       " '최고': 773,\n",
       " '▁손': 774,\n",
       " '글': 775,\n",
       " '낸': 776,\n",
       " '브': 777,\n",
       " '▁영화라고': 778,\n",
       " '른': 779,\n",
       " '▁시리즈': 780,\n",
       " '▁생': 781,\n",
       " '▁날': 782,\n",
       " '평점': 783,\n",
       " '▁좋아하는': 784,\n",
       " '▁수작': 785,\n",
       " 'ᅳ': 786,\n",
       " '▁아직도': 787,\n",
       " '▁뻔한': 788,\n",
       " '▁시간이': 789,\n",
       " '친': 790,\n",
       " '주는': 791,\n",
       " '악': 792,\n",
       " '▁여운이': 793,\n",
       " '▁매력': 794,\n",
       " '▁유치': 795,\n",
       " '잡': 796,\n",
       " '해도': 797,\n",
       " '▁때문에': 798,\n",
       " '\"\"': 799,\n",
       " '설': 800,\n",
       " '▁차라리': 801,\n",
       " '직': 802,\n",
       " '병': 803,\n",
       " '퍼': 804,\n",
       " '▁나름': 805,\n",
       " '▁하고': 806,\n",
       " '▁재미도': 807,\n",
       " '던데': 808,\n",
       " '싶': 809,\n",
       " '립': 810,\n",
       " '▁좋았다': 811,\n",
       " '짐': 812,\n",
       " '▁명': 813,\n",
       " '투': 814,\n",
       " '/': 815,\n",
       " '갈': 816,\n",
       " '▁재밌어요': 817,\n",
       " '▁들어': 818,\n",
       " '▁영화입니다': 819,\n",
       " '재밌': 820,\n",
       " '▁나온': 821,\n",
       " '▁특히': 822,\n",
       " '▁걍': 823,\n",
       " '동안': 824,\n",
       " '순': 825,\n",
       " '▁장난': 826,\n",
       " '▁도대체': 827,\n",
       " '▁한국영화': 828,\n",
       " '▁행복': 829,\n",
       " '점은': 830,\n",
       " '▁주는': 831,\n",
       " '▁되는': 832,\n",
       " '▁사실': 833,\n",
       " '▁영화의': 834,\n",
       " '▁진심': 835,\n",
       " '너': 836,\n",
       " '잼': 837,\n",
       " '있다': 838,\n",
       " '강': 839,\n",
       " '▁스토리도': 840,\n",
       " '▁우리나라': 841,\n",
       " '▁같': 842,\n",
       " '천': 843,\n",
       " '락': 844,\n",
       " '었던': 845,\n",
       " '중에': 846,\n",
       " '같': 847,\n",
       " '었음': 848,\n",
       " '▁나도': 849,\n",
       " '▁보면서': 850,\n",
       " '▁타': 851,\n",
       " '할수': 852,\n",
       " '▁ᄏᄏᄏᄏ': 853,\n",
       " '▁감독의': 854,\n",
       " '▁B': 855,\n",
       " '▁오랜만에': 856,\n",
       " '▁될': 857,\n",
       " '6': 858,\n",
       " ';;;': 859,\n",
       " '▁당': 860,\n",
       " '좋': 861,\n",
       " '▁멋진': 862,\n",
       " '▁따': 863,\n",
       " '▁잼있': 864,\n",
       " '▁몇': 865,\n",
       " '▁기분': 866,\n",
       " '▁마지막에': 867,\n",
       " 'h': 868,\n",
       " '▁치': 869,\n",
       " 'ᅮᅮ': 870,\n",
       " '블': 871,\n",
       " '▁답답': 872,\n",
       " '▁있었': 873,\n",
       " '록': 874,\n",
       " '▁역': 875,\n",
       " '▁담': 876,\n",
       " '▁공포영화': 877,\n",
       " '존': 878,\n",
       " '???': 879,\n",
       " '졌': 880,\n",
       " '▁이해가': 881,\n",
       " '잔': 882,\n",
       " '월': 883,\n",
       " '효': 884,\n",
       " '답': 885,\n",
       " '▁첨': 886,\n",
       " '된다': 887,\n",
       " '겠': 888,\n",
       " '외': 889,\n",
       " '▁봤습니다': 890,\n",
       " '▁짜증': 891,\n",
       " '보단': 892,\n",
       " '▁OOO': 893,\n",
       " '▁머': 894,\n",
       " '습': 895,\n",
       " '▁감정': 896,\n",
       " '김': 897,\n",
       " '▁경': 898,\n",
       " '▁봤던': 899,\n",
       " '지않': 900,\n",
       " '▁정신': 901,\n",
       " '▁봐야': 902,\n",
       " '▁보게': 903,\n",
       " '초': 904,\n",
       " '9': 905,\n",
       " '이런': 906,\n",
       " '레이': 907,\n",
       " '▁없음': 908,\n",
       " '▁알바': 909,\n",
       " '▁재미가': 910,\n",
       " '▁전쟁': 911,\n",
       " '놈': 912,\n",
       " '▁호': 913,\n",
       " '▁뭔': 914,\n",
       " '나는': 915,\n",
       " '▁어떤': 916,\n",
       " '▁좋고': 917,\n",
       " '▁열': 918,\n",
       " '▁지루하고': 919,\n",
       " '▁시나리오': 920,\n",
       " '재미': 921,\n",
       " '▁지루함': 922,\n",
       " '▁필요': 923,\n",
       " '탄': 924,\n",
       " '▁지루한': 925,\n",
       " '복': 926,\n",
       " '▁영화에': 927,\n",
       " '봤다': 928,\n",
       " '▁생각이': 929,\n",
       " '▁싶은': 930,\n",
       " 'g': 931,\n",
       " '▁분위기': 932,\n",
       " '쓰레기': 933,\n",
       " '그냥': 934,\n",
       " '엇': 935,\n",
       " '▁사람이': 936,\n",
       " '머': 937,\n",
       " '교': 938,\n",
       " '▁웃기': 939,\n",
       " '00': 940,\n",
       " '▁이딴': 941,\n",
       " '▁마음': 942,\n",
       " '라서': 943,\n",
       " '▁오늘': 944,\n",
       " '▁재밌는': 945,\n",
       " '▁토': 946,\n",
       " '덕': 947,\n",
       " '▁함께': 948,\n",
       " '창': 949,\n",
       " '▁뭘': 950,\n",
       " '▁감동적이': 951,\n",
       " '하며': 952,\n",
       " '▁그저': 953,\n",
       " '▁화': 954,\n",
       " '하는데': 955,\n",
       " '▁문제': 956,\n",
       " '지루': 957,\n",
       " '해야': 958,\n",
       " '▁어디': 959,\n",
       " '>': 960,\n",
       " '▁결국': 961,\n",
       " '새': 962,\n",
       " '이지': 963,\n",
       " ',,,': 964,\n",
       " '▁만화': 965,\n",
       " '▁배우들의': 966,\n",
       " '했음': 967,\n",
       " '▁많': 968,\n",
       " '▁졸작': 969,\n",
       " '에는': 970,\n",
       " '사랑': 971,\n",
       " '찬': 972,\n",
       " '▁힘': 973,\n",
       " '▁괜찮은': 974,\n",
       " '광': 975,\n",
       " '에도': 976,\n",
       " '기에': 977,\n",
       " '온': 978,\n",
       " '▁개인적으로': 979,\n",
       " '▁최고다': 980,\n",
       " '▁큰': 981,\n",
       " '허': 982,\n",
       " '▁설정': 983,\n",
       " '치고': 984,\n",
       " '짜': 985,\n",
       " '기는': 986,\n",
       " '법': 987,\n",
       " '▁역사': 988,\n",
       " '점대': 989,\n",
       " 'm': 990,\n",
       " '▁올': 991,\n",
       " '▁점수': 992,\n",
       " '▁ᅲ': 993,\n",
       " '~!': 994,\n",
       " '아서': 995,\n",
       " '▁보는내내': 996,\n",
       " '보니': 997,\n",
       " '▁추': 998,\n",
       " '▁애니메이션': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('naver.vocab', encoding='utf-8') as f:\n",
    "    Vo = [doc.strip().split(\"\\t\") for doc in f]\n",
    "\n",
    "# w[0]: Padding   \n",
    "# w[1]: Unkown\n",
    "word2idx = {w[0]: i for i, w in enumerate(Vo)}\n",
    "word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 준단어 토큰화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁아', '▁더빙', '..', '▁진짜', '▁짜증나', '네요', '▁목소리']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('naver.model') \n",
    "\n",
    "sp.encode_as_pieces(nsmc.loc[0, 'document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=5000, lowercase=False, tokenizer=sp.encode_as_pieces)\n",
    "tdm = cv.fit_transform(nsmc['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test 분리\n",
    "x_train = tdm[:1600]\n",
    "x_test = tdm[1600:]\n",
    "y_train = nsmc['label'][:1600]\n",
    "y_test = nsmc['label'][1600:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP 분석 (Keras의 Dense Layer 활용) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. sentencepiece 활용 할때\n",
    "\n",
    "2. stanfordnlp 사용할 때 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentencePiece 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(1, input_shape=(5000,), activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
      "Train on 1600 samples\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 1s 473us/sample - loss: 0.6874 - accuracy: 0.5569\n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 0s 144us/sample - loss: 0.6572 - accuracy: 0.7513\n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 0s 143us/sample - loss: 0.6308 - accuracy: 0.8269\n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 0s 142us/sample - loss: 0.6063 - accuracy: 0.8644\n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 0s 139us/sample - loss: 0.5841 - accuracy: 0.8831\n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 0s 145us/sample - loss: 0.5637 - accuracy: 0.8931\n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 0s 145us/sample - loss: 0.5449 - accuracy: 0.9038\n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 0s 143us/sample - loss: 0.5273 - accuracy: 0.9100\n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 0s 145us/sample - loss: 0.5111 - accuracy: 0.9137\n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 0s 143us/sample - loss: 0.4958 - accuracy: 0.9175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18d72bd2a48>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
      "148395/148395 [==============================] - 17s 114us/sample - loss: 0.6065 - accuracy: 0.7341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6064644928593701, 0.73412174]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight, bias = model.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_weight = pd.DataFrame({\n",
    "    '단어': cv.get_feature_names(),\n",
    "    '가중치': weight.numpy().flat\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>단어</th>\n",
       "      <th>가중치</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2891</td>\n",
       "      <td>냐</td>\n",
       "      <td>-0.257413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1428</td>\n",
       "      <td>▁아까운</td>\n",
       "      <td>-0.238297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3586</td>\n",
       "      <td>분</td>\n",
       "      <td>-0.212414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>▁0</td>\n",
       "      <td>-0.206012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1752</td>\n",
       "      <td>▁욕</td>\n",
       "      <td>-0.205945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        단어       가중치\n",
       "2891     냐 -0.257413\n",
       "1428  ▁아까운 -0.238297\n",
       "3586     분 -0.212414\n",
       "235     ▁0 -0.206012\n",
       "1752    ▁욕 -0.205945"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_weight.sort_values('가중치').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>단어</th>\n",
       "      <th>가중치</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2058</td>\n",
       "      <td>▁정말</td>\n",
       "      <td>0.215537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2262</td>\n",
       "      <td>▁최고</td>\n",
       "      <td>0.215565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>922</td>\n",
       "      <td>▁명작</td>\n",
       "      <td>0.220116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1997</td>\n",
       "      <td>▁재밌게</td>\n",
       "      <td>0.233178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2264</td>\n",
       "      <td>▁최고의</td>\n",
       "      <td>0.248644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        단어       가중치\n",
       "2058   ▁정말  0.215537\n",
       "2262   ▁최고  0.215565\n",
       "922    ▁명작  0.220116\n",
       "1997  ▁재밌게  0.233178\n",
       "2264  ▁최고의  0.248644"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_weight.sort_values('가중치').tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stanfordnlp 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: gpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\hyoun\\\\stanfordnlp_resources\\\\ko_gsd_models\\\\ko_gsd_tokenizer.pt', 'lang': 'ko', 'shorthand': 'ko_gsd', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\hyoun\\\\stanfordnlp_resources\\\\ko_gsd_models\\\\ko_gsd_tagger.pt', 'pretrain_path': 'C:\\\\Users\\\\hyoun\\\\stanfordnlp_resources\\\\ko_gsd_models\\\\ko_gsd.pretrain.pt', 'lang': 'ko', 'shorthand': 'ko_gsd', 'mode': 'predict'}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 4.00 GiB total capacity; 1.03 MiB already allocated; 0 bytes free; 997.50 KiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-d90bbb33f4b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstanfordnlp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstanfordnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ko'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtreebank\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ko_gsd'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stanfordnlp\\pipeline\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, processors, lang, models_dir, treebank, use_gpu, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m                 self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,\n\u001b[0;32m    122\u001b[0m                                                                                           \u001b[0mpipeline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                                                                                           use_gpu=self.use_gpu)\n\u001b[0m\u001b[0;32m    124\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mProcessorRequirementsException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m                 \u001b[1;31m# if there was a requirements issue, add it to list which will be printed at end\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stanfordnlp\\pipeline\\processor.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config, pipeline, use_gpu)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_up_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;31m# run set up process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;31m# build the final config for the processor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stanfordnlp\\pipeline\\pos_processor.py\u001b[0m in \u001b[0;36m_set_up_model\u001b[1;34m(self, config, use_gpu)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pretrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPretrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pretrain_path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# set up trainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stanfordnlp\\models\\pos\\trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, vocab, pretrain, model_file, use_cuda)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_cuda\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mcuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m         \"\"\"\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    228\u001b[0m                 \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m                     \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m         \"\"\"\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 4.00 GiB total capacity; 1.03 MiB already allocated; 0 bytes free; 997.50 KiB cached)"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "\n",
    "nlp = stanfordnlp.Pipeline(lang='ko', treebank='ko_gsd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = nsmc['document'][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_token(text): # 명사추출 함수 \n",
    "    doc = nlp(text)\n",
    "    token = []\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            token.append(word.text)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_token(nsmc.loc[0, 'document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=5000, tokenizer= extract_token)\n",
    "\n",
    "tdm = cv.fit_transform(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 코드 작성 중 자꾸 오류가 발생하여 진행하지 못하였음. 이와 비슷한 방식으로 진행하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이런식으로 댓글이 새로 입력이 되었을 때 해당 댓글이 긍정인지 부정인지에 대하여 판별하는 모델을 만들 수 있음 \n",
    "\n",
    "## 단 주의해야 하는 점 NLP의 경우 데이터가 어디에 수집되었느냐에 따라 각각의 의미가 달라질 수 있기 때문에 댓글 활용하여 모델을 만들었다면 댓글 분석에만 사용해야한다. \n",
    "\n",
    "## TDM을 만들어 분석한것 처럼 TDM자리에 TF-IDF를 만들고 분석을 하면 된다. \n",
    "\n",
    "## Seq2Seq등과 같은 다른 기법의 경우 이와 다른 방법을 활용한다. - 궁금한 분께서는 개인적으로 이야기해주세요.\n",
    "\n",
    "## RNN, LSTM 등을 활용하여 분석을 진행할 수 있다 이때는 다른 방법을 사용하는데 순서는 다음과 같다.\n",
    "1. 단어 사전을 제작한다. \n",
    "\n",
    "\n",
    "2. 문장에 대하여 Token화를 진행한 후 해당 단어에 대하여 단어사전에 등록된 숫자로 변경한다. \n",
    "\n",
    "\n",
    "3. 변환된 값을 Embedding에 뿌려 값을 만들고 작업을 진행한다 \n",
    "\n",
    "\n",
    "4. RNN, LSTM, 양방향 RNN 등을 삽입하거나 Flatten하여 작업을 진행한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA, LSA 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Release Year</th>\n",
       "      <th>Title</th>\n",
       "      <th>Origin/Ethnicity</th>\n",
       "      <th>Director</th>\n",
       "      <th>Cast</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Wiki Page</th>\n",
       "      <th>Plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1901</td>\n",
       "      <td>Kansas Saloon Smashers</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Kansas_Saloon_Sm...</td>\n",
       "      <td>A bartender is working at a saloon, serving dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1901</td>\n",
       "      <td>Love by the Light of the Moon</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Love_by_the_Ligh...</td>\n",
       "      <td>The moon, painted with a smiling face hangs ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1901</td>\n",
       "      <td>The Martyred Presidents</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Martyred_Pre...</td>\n",
       "      <td>The film, just over a minute long, is composed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1901</td>\n",
       "      <td>Terrible Teddy, the Grizzly King</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Terrible_Teddy,_...</td>\n",
       "      <td>Lasting just 61 seconds and consisting of two ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1902</td>\n",
       "      <td>Jack and the Beanstalk</td>\n",
       "      <td>American</td>\n",
       "      <td>George S. Fleming, Edwin S. Porter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Jack_and_the_Bea...</td>\n",
       "      <td>The earliest known adaptation of the classic f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Release Year                             Title Origin/Ethnicity  \\\n",
       "0          1901            Kansas Saloon Smashers         American   \n",
       "1          1901     Love by the Light of the Moon         American   \n",
       "2          1901           The Martyred Presidents         American   \n",
       "3          1901  Terrible Teddy, the Grizzly King         American   \n",
       "4          1902            Jack and the Beanstalk         American   \n",
       "\n",
       "                             Director Cast    Genre  \\\n",
       "0                             Unknown  NaN  unknown   \n",
       "1                             Unknown  NaN  unknown   \n",
       "2                             Unknown  NaN  unknown   \n",
       "3                             Unknown  NaN  unknown   \n",
       "4  George S. Fleming, Edwin S. Porter  NaN  unknown   \n",
       "\n",
       "                                           Wiki Page  \\\n",
       "0  https://en.wikipedia.org/wiki/Kansas_Saloon_Sm...   \n",
       "1  https://en.wikipedia.org/wiki/Love_by_the_Ligh...   \n",
       "2  https://en.wikipedia.org/wiki/The_Martyred_Pre...   \n",
       "3  https://en.wikipedia.org/wiki/Terrible_Teddy,_...   \n",
       "4  https://en.wikipedia.org/wiki/Jack_and_the_Bea...   \n",
       "\n",
       "                                                Plot  \n",
       "0  A bartender is working at a saloon, serving dr...  \n",
       "1  The moon, painted with a smiling face hangs ov...  \n",
       "2  The film, just over a minute long, is composed...  \n",
       "3  Lasting just 61 seconds and consisting of two ...  \n",
       "4  The earliest known adaptation of the classic f...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('wikipedia-movie-plots.zip')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = df['Plot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "plot_train, plot_test = train_test_split(plot, test_size=.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english', max_features=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = cv.fit_transform(plot_train)\n",
    "x_test = cv.transform(plot_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim을 이용한 LDA\n",
    "\n",
    "### gensim 설치\n",
    "\n",
    "아나콘다에서 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\hyoun\\Anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - gensim\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    boto3-1.10.19              |             py_0          91 KB\n",
      "    botocore-1.13.19           |             py_0         3.3 MB\n",
      "    bz2file-0.98               |           py37_1          14 KB\n",
      "    gensim-3.8.0               |   py37hf9181ef_0        18.4 MB\n",
      "    jmespath-0.9.4             |             py_0          22 KB\n",
      "    s3transfer-0.2.1           |           py37_0          99 KB\n",
      "    smart_open-1.9.0           |             py_0          59 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        22.0 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  boto3              pkgs/main/noarch::boto3-1.10.19-py_0\n",
      "  botocore           pkgs/main/noarch::botocore-1.13.19-py_0\n",
      "  bz2file            pkgs/main/win-64::bz2file-0.98-py37_1\n",
      "  gensim             pkgs/main/win-64::gensim-3.8.0-py37hf9181ef_0\n",
      "  jmespath           pkgs/main/noarch::jmespath-0.9.4-py_0\n",
      "  s3transfer         pkgs/main/win-64::s3transfer-0.2.1-py37_0\n",
      "  smart_open         pkgs/main/noarch::smart_open-1.9.0-py_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "smart_open-1.9.0     | 59 KB     |            |   0% \n",
      "smart_open-1.9.0     | 59 KB     | ########## | 100% \n",
      "\n",
      "boto3-1.10.19        | 91 KB     |            |   0% \n",
      "boto3-1.10.19        | 91 KB     | ########## | 100% \n",
      "\n",
      "gensim-3.8.0         | 18.4 MB   |            |   0% \n",
      "gensim-3.8.0         | 18.4 MB   |            |   0% \n",
      "gensim-3.8.0         | 18.4 MB   | 1          |   1% \n",
      "gensim-3.8.0         | 18.4 MB   | 2          |   2% \n",
      "gensim-3.8.0         | 18.4 MB   | 3          |   3% \n",
      "gensim-3.8.0         | 18.4 MB   | 4          |   4% \n",
      "gensim-3.8.0         | 18.4 MB   | 5          |   5% \n",
      "gensim-3.8.0         | 18.4 MB   | 6          |   6% \n",
      "gensim-3.8.0         | 18.4 MB   | 7          |   7% \n",
      "gensim-3.8.0         | 18.4 MB   | 8          |   8% \n",
      "gensim-3.8.0         | 18.4 MB   | 9          |   9% \n",
      "gensim-3.8.0         | 18.4 MB   | 9          |  10% \n",
      "gensim-3.8.0         | 18.4 MB   | #          |  11% \n",
      "gensim-3.8.0         | 18.4 MB   | #1         |  12% \n",
      "gensim-3.8.0         | 18.4 MB   | #2         |  13% \n",
      "gensim-3.8.0         | 18.4 MB   | #3         |  14% \n",
      "gensim-3.8.0         | 18.4 MB   | #4         |  15% \n",
      "gensim-3.8.0         | 18.4 MB   | #5         |  16% \n",
      "gensim-3.8.0         | 18.4 MB   | #6         |  17% \n",
      "gensim-3.8.0         | 18.4 MB   | #7         |  18% \n",
      "gensim-3.8.0         | 18.4 MB   | #8         |  19% \n",
      "gensim-3.8.0         | 18.4 MB   | #9         |  20% \n",
      "gensim-3.8.0         | 18.4 MB   | ##         |  21% \n",
      "gensim-3.8.0         | 18.4 MB   | ##1        |  22% \n",
      "gensim-3.8.0         | 18.4 MB   | ##2        |  23% \n",
      "gensim-3.8.0         | 18.4 MB   | ##3        |  23% \n",
      "gensim-3.8.0         | 18.4 MB   | ##4        |  24% \n",
      "gensim-3.8.0         | 18.4 MB   | ##5        |  25% \n",
      "gensim-3.8.0         | 18.4 MB   | ##6        |  26% \n",
      "gensim-3.8.0         | 18.4 MB   | ##7        |  27% \n",
      "gensim-3.8.0         | 18.4 MB   | ##8        |  28% \n",
      "gensim-3.8.0         | 18.4 MB   | ##9        |  29% \n",
      "gensim-3.8.0         | 18.4 MB   | ###        |  30% \n",
      "gensim-3.8.0         | 18.4 MB   | ###1       |  31% \n",
      "gensim-3.8.0         | 18.4 MB   | ###2       |  32% \n",
      "gensim-3.8.0         | 18.4 MB   | ###2       |  33% \n",
      "gensim-3.8.0         | 18.4 MB   | ###3       |  34% \n",
      "gensim-3.8.0         | 18.4 MB   | ###5       |  35% \n",
      "gensim-3.8.0         | 18.4 MB   | ###6       |  36% \n",
      "gensim-3.8.0         | 18.4 MB   | ###7       |  37% \n",
      "gensim-3.8.0         | 18.4 MB   | ###8       |  38% \n",
      "gensim-3.8.0         | 18.4 MB   | ###9       |  39% \n",
      "gensim-3.8.0         | 18.4 MB   | ####       |  40% \n",
      "gensim-3.8.0         | 18.4 MB   | ####       |  41% \n",
      "gensim-3.8.0         | 18.4 MB   | ####1      |  42% \n",
      "gensim-3.8.0         | 18.4 MB   | ####2      |  43% \n",
      "gensim-3.8.0         | 18.4 MB   | ####3      |  44% \n",
      "gensim-3.8.0         | 18.4 MB   | ####4      |  45% \n",
      "gensim-3.8.0         | 18.4 MB   | ####5      |  46% \n",
      "gensim-3.8.0         | 18.4 MB   | ####6      |  46% \n",
      "gensim-3.8.0         | 18.4 MB   | ####7      |  47% \n",
      "gensim-3.8.0         | 18.4 MB   | ####7      |  48% \n",
      "gensim-3.8.0         | 18.4 MB   | ####8      |  49% \n",
      "gensim-3.8.0         | 18.4 MB   | ####9      |  49% \n",
      "gensim-3.8.0         | 18.4 MB   | #####      |  50% \n",
      "gensim-3.8.0         | 18.4 MB   | #####1     |  51% \n",
      "gensim-3.8.0         | 18.4 MB   | #####2     |  52% \n",
      "gensim-3.8.0         | 18.4 MB   | #####2     |  53% \n",
      "gensim-3.8.0         | 18.4 MB   | #####3     |  54% \n",
      "gensim-3.8.0         | 18.4 MB   | #####4     |  55% \n",
      "gensim-3.8.0         | 18.4 MB   | #####5     |  56% \n",
      "gensim-3.8.0         | 18.4 MB   | #####6     |  57% \n",
      "gensim-3.8.0         | 18.4 MB   | #####7     |  57% \n",
      "gensim-3.8.0         | 18.4 MB   | #####8     |  58% \n",
      "gensim-3.8.0         | 18.4 MB   | #####9     |  59% \n",
      "gensim-3.8.0         | 18.4 MB   | ######     |  60% \n",
      "gensim-3.8.0         | 18.4 MB   | ######1    |  61% \n",
      "gensim-3.8.0         | 18.4 MB   | ######2    |  62% \n",
      "gensim-3.8.0         | 18.4 MB   | ######3    |  63% \n",
      "gensim-3.8.0         | 18.4 MB   | ######4    |  64% \n",
      "gensim-3.8.0         | 18.4 MB   | ######5    |  65% \n",
      "gensim-3.8.0         | 18.4 MB   | ######6    |  66% \n",
      "gensim-3.8.0         | 18.4 MB   | ######7    |  67% \n",
      "gensim-3.8.0         | 18.4 MB   | ######8    |  68% \n",
      "gensim-3.8.0         | 18.4 MB   | ######9    |  69% \n",
      "gensim-3.8.0         | 18.4 MB   | #######    |  70% \n",
      "gensim-3.8.0         | 18.4 MB   | #######1   |  71% \n",
      "gensim-3.8.0         | 18.4 MB   | #######2   |  72% \n",
      "gensim-3.8.0         | 18.4 MB   | #######3   |  73% \n",
      "gensim-3.8.0         | 18.4 MB   | #######4   |  74% \n",
      "gensim-3.8.0         | 18.4 MB   | #######5   |  75% \n",
      "gensim-3.8.0         | 18.4 MB   | #######6   |  76% \n",
      "gensim-3.8.0         | 18.4 MB   | #######7   |  77% \n",
      "gensim-3.8.0         | 18.4 MB   | #######8   |  78% \n",
      "gensim-3.8.0         | 18.4 MB   | #######9   |  79% \n",
      "gensim-3.8.0         | 18.4 MB   | ########   |  80% \n",
      "gensim-3.8.0         | 18.4 MB   | ########1  |  81% \n",
      "gensim-3.8.0         | 18.4 MB   | ########2  |  82% \n",
      "gensim-3.8.0         | 18.4 MB   | ########3  |  83% \n",
      "gensim-3.8.0         | 18.4 MB   | ########4  |  84% \n",
      "gensim-3.8.0         | 18.4 MB   | ########5  |  85% \n",
      "gensim-3.8.0         | 18.4 MB   | ########6  |  86% \n",
      "gensim-3.8.0         | 18.4 MB   | ########7  |  87% \n",
      "gensim-3.8.0         | 18.4 MB   | ########8  |  88% \n",
      "gensim-3.8.0         | 18.4 MB   | ########8  |  89% \n",
      "gensim-3.8.0         | 18.4 MB   | ########9  |  90% \n",
      "gensim-3.8.0         | 18.4 MB   | #########  |  91% \n",
      "gensim-3.8.0         | 18.4 MB   | #########1 |  92% \n",
      "gensim-3.8.0         | 18.4 MB   | #########2 |  93% \n",
      "gensim-3.8.0         | 18.4 MB   | #########3 |  94% \n",
      "gensim-3.8.0         | 18.4 MB   | #########4 |  95% \n",
      "gensim-3.8.0         | 18.4 MB   | #########5 |  95% \n",
      "gensim-3.8.0         | 18.4 MB   | #########5 |  96% \n",
      "gensim-3.8.0         | 18.4 MB   | #########6 |  97% \n",
      "gensim-3.8.0         | 18.4 MB   | #########7 |  97% \n",
      "gensim-3.8.0         | 18.4 MB   | #########8 |  98% \n",
      "gensim-3.8.0         | 18.4 MB   | #########9 |  99% \n",
      "gensim-3.8.0         | 18.4 MB   | #########9 | 100% \n",
      "gensim-3.8.0         | 18.4 MB   | ########## | 100% \n",
      "\n",
      "bz2file-0.98         | 14 KB     |            |   0% \n",
      "bz2file-0.98         | 14 KB     | ########## | 100% \n",
      "\n",
      "s3transfer-0.2.1     | 99 KB     |            |   0% \n",
      "s3transfer-0.2.1     | 99 KB     | ########## | 100% \n",
      "\n",
      "jmespath-0.9.4       | 22 KB     |            |   0% \n",
      "jmespath-0.9.4       | 22 KB     | ########## | 100% \n",
      "\n",
      "botocore-1.13.19     | 3.3 MB    |            |   0% \n",
      "botocore-1.13.19     | 3.3 MB    | 4          |   4% \n",
      "botocore-1.13.19     | 3.3 MB    | 9          |   9% \n",
      "botocore-1.13.19     | 3.3 MB    | #5         |  15% \n",
      "botocore-1.13.19     | 3.3 MB    | ##         |  21% \n",
      "botocore-1.13.19     | 3.3 MB    | ##5        |  26% \n",
      "botocore-1.13.19     | 3.3 MB    | ###        |  31% \n",
      "botocore-1.13.19     | 3.3 MB    | ###6       |  37% \n",
      "botocore-1.13.19     | 3.3 MB    | ####2      |  42% \n",
      "botocore-1.13.19     | 3.3 MB    | ####7      |  47% \n",
      "botocore-1.13.19     | 3.3 MB    | #####3     |  53% \n",
      "botocore-1.13.19     | 3.3 MB    | #####8     |  58% \n",
      "botocore-1.13.19     | 3.3 MB    | ######3    |  63% \n",
      "botocore-1.13.19     | 3.3 MB    | ######9    |  69% \n",
      "botocore-1.13.19     | 3.3 MB    | #######4   |  75% \n",
      "botocore-1.13.19     | 3.3 MB    | #######9   |  80% \n",
      "botocore-1.13.19     | 3.3 MB    | ########5  |  85% \n",
      "botocore-1.13.19     | 3.3 MB    | #########  |  91% \n",
      "botocore-1.13.19     | 3.3 MB    | #########5 |  96% \n",
      "botocore-1.13.19     | 3.3 MB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "!conda install -y gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아나콘다가 아닐 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TDM을 gensim의 Corpus 포맷으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import Sparse2Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Sparse2Corpus(x_train.T) # x_train.T에서 .T는 전치(transpose: 행과 열을 뒤집는 것)\n",
    "# 행에 단어 컬럼에 문장을 넣어야 함. (문장을 묶어 주제로 만들기 위해 컬럼으로 들어가야함.)\n",
    "# gensim 패키지는 tdm을 넣어주면 된당. \n",
    "# corpus 포맷으로 만들어 주어야 실행된다. \n",
    "# corpus 형태는 행에 단어 열에 주제에 대한 정보 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dict(enumerate(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA 분석 실시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "# 주제가 몇개 있는지 모르기 때문에 하이퍼 파라미터 설정 \n",
    "for num_topics in [5, 10, 50]:\n",
    "    models[num_topics] = LdaModel(\n",
    "        corpus=corpus,\n",
    "        num_topics=num_topics,\n",
    "        passes=3, # passes와 iterations는 연산 횟수를 의미하는데 많이 하면 할 수 록 성능 좋아짐. # passes 전체 데이터를 몇번 훑을지, epoch와 같음  \n",
    "        iterations=100, \n",
    "        id2word=id2word,\n",
    "        random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.141602"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Monte carlo 기법? \n",
    "\n",
    "# 원의 반지름은 어케구하나?\n",
    "## 확률을 추정한 다음 시행을 무한이 많이 하여 수렴하게 하는 기법 이를 통해 미지수를 추정하는 방법임 \n",
    "\n",
    "# 파이 추정하는 방법 \n",
    "# LDA도 이와 같은 기법 활용해서 근사해서 계산함. \n",
    "import numpy as np\n",
    "\n",
    "x = np.random.uniform(-1,1,10000000)\n",
    "y = np.random.uniform(-1,1,10000000)\n",
    "\n",
    "(x ** 2 + y ** 2 <= 1).mean()*4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "\n",
    "단어의 분포가 얼마나 정확한지에 대하여 판단할때 사용하는 수치\n",
    "\n",
    "수치가 커지면 단어 복잡도가 많이 크다는 의미임. \n",
    "\n",
    "0에 가까울 수록 더 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = Sparse2Corpus(x_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: <gensim.models.ldamodel.LdaModel at 0x20d3964a148>,\n",
       " 10: <gensim.models.ldamodel.LdaModel at 0x20d32a89e48>,\n",
       " 50: <gensim.models.ldamodel.LdaModel at 0x20d39436748>}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 -7.108070179948764\n",
      "10 -7.105554355511369\n",
      "50 -7.190093588127462\n"
     ]
    }
   ],
   "source": [
    "for num_topics in models:\n",
    "    print(num_topics, models[num_topics].log_perplexity(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1218.717500269996"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 실제 perplexity 구하기\n",
    "\n",
    "np.exp(7.105554355511369) # 1218단어 마다 1개씩 100% 맞출 수 있다. \n",
    "# 다음단어 추론할 때 1/1218의 확률이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주제 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"war\" + 0.008*\"king\" + 0.006*\"ship\" + 0.006*\"men\" + 0.006*\"group\" + 0.006*\"army\" + 0.006*\"world\" + 0.005*\"killed\" + 0.005*\"escape\" + 0.005*\"captain\"'),\n",
       " (1,\n",
       "  '0.019*\"tom\" + 0.017*\"house\" + 0.010*\"jerry\" + 0.009*\"body\" + 0.009*\"night\" + 0.009*\"room\" + 0.008*\"tells\" + 0.008*\"home\" + 0.008*\"finds\" + 0.007*\"away\"'),\n",
       " (2,\n",
       "  '0.021*\"father\" + 0.021*\"love\" + 0.017*\"family\" + 0.013*\"mother\" + 0.012*\"son\" + 0.011*\"daughter\" + 0.009*\"life\" + 0.009*\"marriage\" + 0.009*\"house\" + 0.008*\"wife\"'),\n",
       " (3,\n",
       "  '0.018*\"police\" + 0.009*\"car\" + 0.007*\"sam\" + 0.007*\"killed\" + 0.007*\"kill\" + 0.007*\"man\" + 0.006*\"escape\" + 0.006*\"gun\" + 0.006*\"jimmy\" + 0.005*\"later\"'),\n",
       " (4,\n",
       "  '0.010*\"school\" + 0.010*\"new\" + 0.008*\"time\" + 0.007*\"film\" + 0.007*\"day\" + 0.007*\"home\" + 0.006*\"tells\" + 0.006*\"friends\" + 0.006*\"life\" + 0.005*\"father\"'),\n",
       " (5,\n",
       "  '0.055*\"frank\" + 0.054*\"charlie\" + 0.046*\"johnny\" + 0.041*\"nick\" + 0.038*\"raja\" + 0.028*\"helen\" + 0.019*\"linda\" + 0.018*\"jackie\" + 0.017*\"anand\" + 0.014*\"kelly\"'),\n",
       " (6,\n",
       "  '0.021*\"police\" + 0.015*\"film\" + 0.009*\"story\" + 0.009*\"murder\" + 0.008*\"killed\" + 0.008*\"kill\" + 0.007*\"man\" + 0.007*\"life\" + 0.007*\"case\" + 0.006*\"gang\"'),\n",
       " (7,\n",
       "  '0.018*\"john\" + 0.012*\"mary\" + 0.012*\"david\" + 0.009*\"peter\" + 0.009*\"michael\" + 0.008*\"wife\" + 0.008*\"mrs\" + 0.008*\"jim\" + 0.007*\"henry\" + 0.007*\"new\"'),\n",
       " (8,\n",
       "  '0.045*\"paul\" + 0.037*\"mike\" + 0.036*\"george\" + 0.025*\"jeff\" + 0.021*\"kate\" + 0.020*\"jake\" + 0.018*\"larry\" + 0.018*\"terry\" + 0.018*\"arthur\" + 0.017*\"andy\"'),\n",
       " (9,\n",
       "  '0.023*\"jack\" + 0.020*\"joe\" + 0.017*\"town\" + 0.014*\"money\" + 0.008*\"danny\" + 0.008*\"billy\" + 0.008*\"anna\" + 0.007*\"train\" + 0.007*\"men\" + 0.006*\"alice\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[10].show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('war', 0.007855945),\n",
       " ('king', 0.0077317017),\n",
       " ('ship', 0.006199054),\n",
       " ('men', 0.006128602),\n",
       " ('group', 0.0057548573),\n",
       " ('army', 0.0055977134),\n",
       " ('world', 0.005520351),\n",
       " ('killed', 0.0051553347),\n",
       " ('escape', 0.0049308473),\n",
       " ('captain', 0.0049248543)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[10].show_topic(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서의 주제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 1),\n",
       " (30, 1),\n",
       " (78, 1),\n",
       " (110, 1),\n",
       " (112, 1),\n",
       " (129, 2),\n",
       " (131, 2),\n",
       " (189, 1),\n",
       " (260, 1),\n",
       " (261, 1),\n",
       " (331, 2),\n",
       " (347, 1),\n",
       " (352, 1),\n",
       " (356, 2),\n",
       " (413, 2),\n",
       " (415, 1),\n",
       " (426, 1),\n",
       " (434, 1),\n",
       " (437, 1),\n",
       " (448, 1),\n",
       " (476, 1),\n",
       " (496, 1),\n",
       " (503, 1),\n",
       " (534, 1),\n",
       " (585, 1),\n",
       " (638, 1),\n",
       " (644, 2),\n",
       " (657, 1),\n",
       " (743, 1),\n",
       " (767, 1),\n",
       " (798, 1),\n",
       " (843, 1),\n",
       " (907, 5),\n",
       " (937, 1),\n",
       " (1023, 1),\n",
       " (1055, 1),\n",
       " (1071, 4),\n",
       " (1075, 1),\n",
       " (1111, 1),\n",
       " (1133, 1),\n",
       " (1151, 1),\n",
       " (1181, 2),\n",
       " (1188, 3),\n",
       " (1206, 1),\n",
       " (1211, 1),\n",
       " (1215, 1),\n",
       " (1235, 1),\n",
       " (1244, 1),\n",
       " (1248, 1),\n",
       " (1267, 2),\n",
       " (1277, 1),\n",
       " (1293, 1),\n",
       " (1399, 1),\n",
       " (1485, 1),\n",
       " (1495, 1),\n",
       " (1536, 1),\n",
       " (1545, 5),\n",
       " (1551, 1),\n",
       " (1573, 1),\n",
       " (1621, 4),\n",
       " (1668, 2),\n",
       " (1671, 1),\n",
       " (1744, 1),\n",
       " (1844, 1),\n",
       " (1867, 1),\n",
       " (1884, 1),\n",
       " (1971, 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = x_test[0]\n",
    "doc = list(zip(row.indices, row.data))\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aboard 1\n",
      "adult 1\n",
      "animals 1\n",
      "arrive 1\n",
      "arrives 1\n",
      "attack 2\n",
      "attacks 2\n",
      "better 1\n",
      "captain 1\n",
      "capture 1\n",
      "circus 2\n",
      "coast 1\n",
      "collect 1\n",
      "comes 2\n",
      "creature 2\n",
      "crew 1\n",
      "cut 1\n",
      "dangerous 1\n",
      "dark 1\n",
      "deal 1\n",
      "despite 1\n",
      "died 1\n",
      "discover 1\n",
      "drive 1\n",
      "enter 1\n",
      "fear 1\n",
      "feet 2\n",
      "finally 1\n",
      "goes 1\n",
      "group 1\n",
      "having 1\n",
      "hoping 1\n",
      "island 5\n",
      "joe 1\n",
      "later 1\n",
      "like 1\n",
      "london 4\n",
      "looking 1\n",
      "manage 1\n",
      "master 1\n",
      "men 1\n",
      "monster 2\n",
      "mother 3\n",
      "names 1\n",
      "navy 1\n",
      "nearly 1\n",
      "note 1\n",
      "offered 1\n",
      "officer 1\n",
      "owner 2\n",
      "park 1\n",
      "people 1\n",
      "public 1\n",
      "rescues 1\n",
      "return 1\n",
      "royal 1\n",
      "ryan 5\n",
      "sam 1\n",
      "sea 1\n",
      "ship 4\n",
      "son 2\n",
      "soon 1\n",
      "study 1\n",
      "treasure 1\n",
      "turns 1\n",
      "university 1\n",
      "woman 1\n"
     ]
    }
   ],
   "source": [
    "for i, n in doc:\n",
    "    print(id2word[i], n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.7228719),\n",
       " (2, 0.06179989),\n",
       " (3, 0.024760846),\n",
       " (8, 0.093357325),\n",
       " (9, 0.09183118)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[10].get_document_topics(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.matrix([[1,2,3],[2,4,5],[6,8,9]])\n",
    "u, s, v = np.linalg.svd(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 2, 3],\n",
       "        [2, 4, 5],\n",
       "        [6, 8, 9]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.23799094, -0.67025772, -0.08246612])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-0.23799094, -0.56130663, -0.79265073],\n",
       "        [-0.43110373, -0.67025772,  0.60407298],\n",
       "        [-0.87035044,  0.48547858, -0.08246612]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15.44287374,  0.        ,  0.        ],\n",
       "       [ 0.        ,  1.21329097,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.21348477]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15.44287374,  1.21329097,  0.21348477])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.40939926,  0.06608843, -0.46739811])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-0.40939926, -0.59336108, -0.69304752],\n",
       "        [ 0.83331158,  0.06608843, -0.5488389 ],\n",
       "        [-0.37146207,  0.80221876, -0.46739811]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1., 2., 3.],\n",
       "        [2., 4., 5.],\n",
       "        [6., 8., 9.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u @ np.diag(s) @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=100, random_state=1234) # 몇 차원으로 줄일지에 대한 이야기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=100, n_iter=5,\n",
       "             random_state=1234, tol=0.0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20d3ba65ac8>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.plot(svd.explained_variance_, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 차원의 개수일 때 설명하는 정도 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 차원 축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6978, 2000)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_emb = svd.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6978, 100)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주제 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2000)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loading = pd.DataFrame(svd.components_.T) # components는 svd를 실행한 결과에서 컬럼에 대한 주제확률 의미한다. 이때 모든 값은 양수  \n",
    "# components_의 값은 열값(여기서는 단어를 의미)을 우리가 지정한 값의 개수만큼으로 늘려 값 변환시킨것. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loading['word'] = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1797</td>\n",
       "      <td>0.190082</td>\n",
       "      <td>tells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>636</td>\n",
       "      <td>0.184727</td>\n",
       "      <td>father</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.143918</td>\n",
       "      <td>house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>838</td>\n",
       "      <td>0.143609</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.135895</td>\n",
       "      <td>man</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0    word\n",
       "1797  0.190082   tells\n",
       "636   0.184727  father\n",
       "850   0.143918   house\n",
       "838   0.143609    home\n",
       "1110  0.135895     man"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "loading.sort_values(i, ascending=False).loc[:, [i, 'word']].head()\n",
    "# 0번째 축에 들어간 단어들을 의미한다. 해당 방향으로 바뀌엇음을 의미. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.017806</td>\n",
       "      <td>-0.005246</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>0.008322</td>\n",
       "      <td>-0.021213</td>\n",
       "      <td>0.011804</td>\n",
       "      <td>0.013197</td>\n",
       "      <td>-0.001181</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.009516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007092</td>\n",
       "      <td>-0.005775</td>\n",
       "      <td>0.003275</td>\n",
       "      <td>0.009648</td>\n",
       "      <td>-0.008887</td>\n",
       "      <td>0.005153</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>-0.008574</td>\n",
       "      <td>0.013057</td>\n",
       "      <td>000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.008293</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>-0.000980</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>-0.004791</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>-0.002099</td>\n",
       "      <td>0.002838</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>-0.001213</td>\n",
       "      <td>0.005679</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>-0.002511</td>\n",
       "      <td>-0.004490</td>\n",
       "      <td>-0.003631</td>\n",
       "      <td>-0.000784</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.004039</td>\n",
       "      <td>-0.000747</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>-0.003508</td>\n",
       "      <td>-0.000539</td>\n",
       "      <td>-0.000865</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.001699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>-0.000848</td>\n",
       "      <td>-0.000898</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>-0.000551</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>-0.001603</td>\n",
       "      <td>-0.000552</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.004470</td>\n",
       "      <td>-0.001072</td>\n",
       "      <td>-0.000567</td>\n",
       "      <td>-0.001102</td>\n",
       "      <td>-0.002203</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>-0.002135</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>-0.002291</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002663</td>\n",
       "      <td>-0.001319</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.004077</td>\n",
       "      <td>-0.002794</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>-0.002016</td>\n",
       "      <td>-0.004333</td>\n",
       "      <td>-0.003633</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.004495</td>\n",
       "      <td>-0.001154</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>-0.002092</td>\n",
       "      <td>0.003453</td>\n",
       "      <td>-0.001529</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>-0.000213</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.003544</td>\n",
       "      <td>-0.000402</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.002282</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>-0.003112</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1995</td>\n",
       "      <td>0.067410</td>\n",
       "      <td>0.016862</td>\n",
       "      <td>-0.012180</td>\n",
       "      <td>-0.017198</td>\n",
       "      <td>0.014854</td>\n",
       "      <td>-0.000783</td>\n",
       "      <td>-0.064824</td>\n",
       "      <td>0.016383</td>\n",
       "      <td>0.015994</td>\n",
       "      <td>-0.002943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012066</td>\n",
       "      <td>0.030562</td>\n",
       "      <td>0.021192</td>\n",
       "      <td>0.054388</td>\n",
       "      <td>0.039986</td>\n",
       "      <td>-0.031969</td>\n",
       "      <td>-0.011626</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.030101</td>\n",
       "      <td>years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1996</td>\n",
       "      <td>0.025387</td>\n",
       "      <td>-0.007917</td>\n",
       "      <td>-0.000599</td>\n",
       "      <td>0.004194</td>\n",
       "      <td>-0.014811</td>\n",
       "      <td>0.046351</td>\n",
       "      <td>-0.034786</td>\n",
       "      <td>0.006689</td>\n",
       "      <td>0.006607</td>\n",
       "      <td>0.003793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015426</td>\n",
       "      <td>-0.001542</td>\n",
       "      <td>-0.010022</td>\n",
       "      <td>0.019866</td>\n",
       "      <td>0.018368</td>\n",
       "      <td>-0.001909</td>\n",
       "      <td>-0.003628</td>\n",
       "      <td>0.022308</td>\n",
       "      <td>0.002969</td>\n",
       "      <td>york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1997</td>\n",
       "      <td>0.067205</td>\n",
       "      <td>-0.015764</td>\n",
       "      <td>-0.016609</td>\n",
       "      <td>-0.019705</td>\n",
       "      <td>-0.001339</td>\n",
       "      <td>-0.004872</td>\n",
       "      <td>-0.053919</td>\n",
       "      <td>0.032678</td>\n",
       "      <td>0.010115</td>\n",
       "      <td>0.017758</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084192</td>\n",
       "      <td>0.068043</td>\n",
       "      <td>0.048618</td>\n",
       "      <td>0.009887</td>\n",
       "      <td>-0.116941</td>\n",
       "      <td>-0.072579</td>\n",
       "      <td>0.061190</td>\n",
       "      <td>0.016748</td>\n",
       "      <td>0.170012</td>\n",
       "      <td>young</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1998</td>\n",
       "      <td>0.016255</td>\n",
       "      <td>-0.000265</td>\n",
       "      <td>-0.005712</td>\n",
       "      <td>-0.009287</td>\n",
       "      <td>0.014262</td>\n",
       "      <td>-0.010788</td>\n",
       "      <td>-0.007070</td>\n",
       "      <td>0.003775</td>\n",
       "      <td>0.006573</td>\n",
       "      <td>-0.004433</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>-0.005062</td>\n",
       "      <td>-0.009501</td>\n",
       "      <td>0.004775</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>-0.014563</td>\n",
       "      <td>0.036915</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.005654</td>\n",
       "      <td>younger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1999</td>\n",
       "      <td>0.005258</td>\n",
       "      <td>-0.001213</td>\n",
       "      <td>-0.001334</td>\n",
       "      <td>-0.003027</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>-0.003450</td>\n",
       "      <td>-0.004869</td>\n",
       "      <td>0.004344</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>-0.000382</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000840</td>\n",
       "      <td>-0.002388</td>\n",
       "      <td>0.005522</td>\n",
       "      <td>-0.002024</td>\n",
       "      <td>-0.003801</td>\n",
       "      <td>0.002227</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>-0.003317</td>\n",
       "      <td>0.003098</td>\n",
       "      <td>youth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.017806 -0.005246  0.001069  0.008322 -0.021213  0.011804  0.013197   \n",
       "1     0.008293 -0.002052 -0.000980  0.000601 -0.004791  0.001792  0.000145   \n",
       "2     0.004039 -0.000747  0.000185  0.000269 -0.003508 -0.000539 -0.000865   \n",
       "3     0.004470 -0.001072 -0.000567 -0.001102 -0.002203  0.001230 -0.002135   \n",
       "4     0.004495 -0.001154  0.000396 -0.000299 -0.002092  0.003453 -0.001529   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1995  0.067410  0.016862 -0.012180 -0.017198  0.014854 -0.000783 -0.064824   \n",
       "1996  0.025387 -0.007917 -0.000599  0.004194 -0.014811  0.046351 -0.034786   \n",
       "1997  0.067205 -0.015764 -0.016609 -0.019705 -0.001339 -0.004872 -0.053919   \n",
       "1998  0.016255 -0.000265 -0.005712 -0.009287  0.014262 -0.010788 -0.007070   \n",
       "1999  0.005258 -0.001213 -0.001334 -0.003027  0.002754 -0.003450 -0.004869   \n",
       "\n",
       "             7         8         9  ...        91        92        93  \\\n",
       "0    -0.001181  0.004836  0.009516  ...  0.007092 -0.005775  0.003275   \n",
       "1     0.001843 -0.002099  0.002838  ...  0.000946  0.000346 -0.001213   \n",
       "2     0.001141  0.000457  0.001699  ...  0.001308 -0.000848 -0.000898   \n",
       "3     0.000707  0.000895 -0.002291  ... -0.002663 -0.001319  0.000589   \n",
       "4     0.001102 -0.000213  0.000043  ...  0.001843  0.003544 -0.000402   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1995  0.016383  0.015994 -0.002943  ...  0.012066  0.030562  0.021192   \n",
       "1996  0.006689  0.006607  0.003793  ...  0.015426 -0.001542 -0.010022   \n",
       "1997  0.032678  0.010115  0.017758  ... -0.084192  0.068043  0.048618   \n",
       "1998  0.003775  0.006573 -0.004433  ...  0.001490 -0.005062 -0.009501   \n",
       "1999  0.004344  0.000901 -0.000382  ... -0.000840 -0.002388  0.005522   \n",
       "\n",
       "            94        95        96        97        98        99     word  \n",
       "0     0.009648 -0.008887  0.005153  0.001257 -0.008574  0.013057      000  \n",
       "1     0.005679  0.002205 -0.002511 -0.004490 -0.003631 -0.000784       10  \n",
       "2     0.001111 -0.000551  0.001025 -0.001603 -0.000552  0.001470      100  \n",
       "3     0.004077 -0.002794  0.000912 -0.002016 -0.004333 -0.003633       12  \n",
       "4     0.000161  0.002282  0.001416 -0.003112  0.002001  0.000053       15  \n",
       "...        ...       ...       ...       ...       ...       ...      ...  \n",
       "1995  0.054388  0.039986 -0.031969 -0.011626  0.002342  0.030101    years  \n",
       "1996  0.019866  0.018368 -0.001909 -0.003628  0.022308  0.002969     york  \n",
       "1997  0.009887 -0.116941 -0.072579  0.061190  0.016748  0.170012    young  \n",
       "1998  0.004775  0.001163 -0.014563  0.036915  0.010742  0.005654  younger  \n",
       "1999 -0.002024 -0.003801  0.002227  0.000696 -0.003317  0.003098    youth  \n",
       "\n",
       "[2000 rows x 101 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loading"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
